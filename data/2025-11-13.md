<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式人工智能的解决方案，用于自动化SNAM能源基础设施的工厂结构获取，通过OCR、视觉LLM、目标检测、关系推理和优化算法，从工厂P&ID图纸中提取结构化设计数据和层次化框架。


<details>
  <summary>Details</summary>
Motivation: 能源转型背景下，需要数字化工具来提升工厂信息获取效率。传统工厂数字化过程依赖人工解读文档，效率低下且易出错，因此需要AI技术来自动化这一过程。

Method: 采用OCR、视觉LLM、目标检测、关系推理和优化算法的协同方法，并扩展了最先进的场景图生成模型，引入新的Transformer架构来深入分析工厂组件间的复杂关系。

Result: 文本信息提取准确率达到91%，组件识别准确率93%，层次结构提取准确率约80%。

Conclusion: 所提出的AI解决方案能够有效克服数据多样性带来的挑战，成功实现了工厂数字化信息的自动化提取，显著提升了工作效率。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [2] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 本文提出了一种系统性的评估方法来分析和量化基于修复的说话人脸生成中的嘴唇泄漏问题，通过三种互补的测试设置和衍生指标来建立更可靠的基准。


<details>
  <summary>Details</summary>
Motivation: 基于修复的说话人脸生成方法在保持视频细节的同时修改嘴唇运动，但容易引入嘴唇泄漏问题，即生成的嘴唇受到参考图像的影响而非仅由驱动音频决定，且难以用标准指标检测。

Method: 提出了包含三种互补测试设置的系统评估框架：静默输入生成、不匹配的音频-视频配对以及匹配的音频-视频合成，并引入了嘴唇同步差异和基于静默音频的嘴唇同步分数等衍生指标。

Result: 该方法能够有效分析和量化嘴唇泄漏问题，并研究了不同身份参考选择对泄漏的影响，为参考设计提供了见解。

Conclusion: 提出的方法是模型无关的，为说话人脸生成领域的未来研究建立了更可靠的基准评估体系。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [3] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: MATRIX是一个多无人机监控数据集和深度学习框架，用于复杂环境下的行人跟踪，解决了动态相机位置和遮挡问题，在挑战性环境中保持约90%的检测跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理动态相机位置和复杂遮挡，需要开发能适应动态无人机监控环境的解决方案。

Method: 提出MATRIX数据集（8个动态位置无人机同步视频）和深度学习框架，包括实时相机标定、特征图像配准、BEV多视图特征融合。

Result: 在复杂环境中保持约90%检测跟踪精度，成功跟踪约80%轨迹，迁移学习表现优异，相机故障时性能优雅下降。

Conclusion: MATRIX数据集和框架为动态多视图监控系统提供了重要基准，展示了在实际部署中的鲁棒性。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [4] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 提出了一种新的持续异常检测框架，使用统一内存库而非任务特定的子内存库，通过核心集增量更新嵌入，在MVTec AD和Visa数据集上达到最先进的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于嵌入的持续异常检测方法需要为每个任务构建特定类别的子内存库，限制了灵活性和可扩展性的问题。

Method: 提出统一内存库框架，所有任务共享一个内存库，通过固定大小的核心集增量更新嵌入，使用最近邻匹配机制计算异常分数。

Result: 在MVTec AD和Visa数据集上分别达到0.972和0.891的平均图像级AUROC分数，在真实电子纸数据集上实现100%异常样本检测准确率。

Conclusion: 该方法在保持高检测精度的同时提高了灵活性和可扩展性，在实际场景中表现出鲁棒性，代码将在GitHub开源。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [5] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出统一框架，结合扩散去噪和时间感知的actor-critic模型，解决自动驾驶事故预测中的传感器噪声和及时可靠预警问题。


<details>
  <summary>Details</summary>
Motivation: 事故预测对主动安全驾驶至关重要，但面临传感器噪声和及时可靠预警的挑战。

Method: 使用扩散模块重建噪声鲁棒的图像和物体特征，结合actor-critic架构进行时间感知的决策，平衡早期检测与可靠性。

Result: 在三个基准数据集上达到最先进精度，显著提高平均事故前时间，在噪声条件下保持鲁棒性能。

Conclusion: 该模型能产生更早、更稳定且符合人类预期的预测，具有实际部署潜力。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [6] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种在潜在空间中实现视觉隐私保护的新方法，通过轻量级匿名化适配器模块从视频特征中移除私人信息，同时保持通用任务效用。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护方法主要关注输入像素级的匿名化，需要重新训练整个视频模型，导致任务特定的匿名化，不适合现代视频基础模型。视频基础模型提取的时空特征在共享或存储时会无意中泄露敏感个人信息。

Method: 引入轻量级匿名化适配器模块（AAM），可即插即用地应用于冻结的视频编码器。设计了三个新的训练目标：片段级自监督隐私目标、协同训练目标和潜在一致性损失。

Result: 评估显示隐私泄露显著减少35%，同时在动作识别、时序动作检测和异常检测等下游任务中保持接近基线的效用性能。有效减轻了动作识别模型中的性别偏见。

Conclusion: 该方法提供了一种高效且通用的视频隐私保护解决方案，能够在保护隐私的同时保持模型在各种任务中的实用性，并促进更公平的视频理解。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [7] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 本文研究了自回归逐像素预测的扩展特性，发现不同视觉任务的最优扩展策略存在显著差异，计算资源而非数据量是主要瓶颈，并预测5年内可实现逐像素图像建模。


<details>
  <summary>Details</summary>
Motivation: 探索自回归逐像素预测这一简单但未被充分研究的统一视觉模型框架的扩展特性，为大规模视觉模型开发提供指导。

Method: 在32x32分辨率下训练一系列Transformer模型，使用IsoFlops配置文件在高达7e19 FLOPs的计算预算下，评估三个目标指标：逐像素预测目标、ImageNet分类准确率和生成质量。

Result: 1) 分类和生成任务的最优扩展策略不同，生成任务需要数据量增长比分类任务快3-5倍；2) 随着分辨率增加，模型大小需比数据量增长更快；3) 主要瓶颈是计算而非训练数据量。

Conclusion: 通过扩展分析发现计算是主要限制因素，基于计算能力每年增长4-5倍的预测，预计5年内可实现逐像素图像建模。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [8] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 本文研究使用扩散模型微调技术（LoRA和DreamBooth）生成平衡训练数据以解决图像分类中的群体偏见问题，通过聚类和分组训练提高生成质量，在多个基准测试中表现优于原始Stable Diffusion并与SOTA去偏技术相当。


<details>
  <summary>Details</summary>
Motivation: 图像分类系统因训练数据中群体代表性不均而继承偏见，如人脸数据集中金发与女性的过度关联。现有方法使用Stable Diffusion生成平衡数据但难以保持原始数据分布，需要改进生成质量。

Method: 探索多种扩散模型微调技术（LoRA和DreamBooth），通过聚类每个群体内的图像并训练每个聚类的DreamBooth模型来处理组内变异，使用这些模型生成群体平衡数据进行预训练，然后在真实数据上微调。

Result: 在多个基准测试中，研究的微调方法平均优于原始Stable Diffusion，与SOTA去偏技术（如Group-DRO）结果相当，且随着数据集偏见严重程度增加而超越这些方法。

Conclusion: 扩散模型微调技术能有效生成更准确的平衡训练数据，缓解图像分类中的群体偏见问题，特别是在严重偏见情况下表现优于现有去偏方法。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [9] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: DT-NVS是一个基于3D扩散模型的广义新视角合成方法，使用transformer架构和仅图像损失在大规模真实世界视频数据集上训练，能够从单张图像生成多样化的新视角。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法主要关注小范围相机移动或仅处理非自然的物体中心场景，限制了在真实世界中的应用。本文旨在解决从单张自然场景图像生成新视角这一未充分探索的问题。

Method: 提出DT-NVS 3D感知扩散模型，采用transformer架构，改进自注意力机制将图像转换为3D表示，引入新颖的相机条件策略，并采用参考帧角色交换的训练范式。

Result: 在单图像广义新视角合成任务上，相比最先进的3D感知扩散模型和确定性方法有显著改进，能够生成多样化的输出。

Conclusion: 该方法成功实现了从单张自然场景图像生成多样化新视角，突破了现有方法的限制，为真实世界应用提供了新的可能性。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [10] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的旋转不变学习方法，通过引入阴影信息姿态特征(SiPF)和旋转不变注意力卷积(RIAttnConv)，解决了现有方法因受限感受野导致的翼尖特征崩溃问题，能够在保持旋转不变性的同时保留全局姿态信息。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变学习方法使用手工制作的旋转不变特征替换原始坐标，但会导致全局姿态信息丢失，无法区分几何相似但空间不同的结构。这种限制源于现有方法的受限感受野，导致无法区分对称组件(如飞机左右机翼)。

Method: 提出阴影信息姿态特征(SiPF)，通过从学习的共享旋转中推导全局一致参考点(称为'阴影')来增强局部旋转不变描述符；提出旋转不变注意力卷积(RIAttnConv)，将SiPF集成到特征聚合过程中；设计基于Bingham分布的任务自适应阴影定位模块，动态学习构建一致阴影的最佳全局旋转。

Result: 在3D分类和部件分割基准测试上的广泛实验表明，该方法显著优于现有旋转不变方法，特别是在需要任意旋转下细粒度空间区分的任务中。

Conclusion: 所提出的方法通过结合局部旋转不变特征和全局姿态信息，有效解决了旋转不变学习中的翼尖特征崩溃问题，在保持旋转不变性的同时提高了对几何相似但空间不同结构的区分能力。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [11] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 本研究开发了一种基于EfficientNet的四步深度学习方法来分类胶质母细胞瘤的6个组织病理学区域，在BraTS-Path 2024挑战数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤预后差，需要自动、稳健且准确的组织病理学区域识别方法来大规模理解该疾病的形态学特征。

Method: 设计了四步深度学习流程，使用EfficientNet架构（B0-B4）对H&E染色组织切片进行六分类，采用5折交叉验证。

Result: EfficientNet-B1和B4在训练集上F1得分达0.98，但在验证集和测试集上分别降至0.546和0.517，显示模型泛化能力挑战。

Conclusion: 开发能够良好泛化到新数据的模型对于临床应用至关重要，本研究为胶质母细胞瘤的自动病理分析提供了基础。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [12] [Improving VisNet for Object Recognition](https://arxiv.org/abs/2511.08897)
*Mehdi Fatan Serj,C. Alejandro Parraga,Xavier Otazu*

Main category: cs.CV

TL;DR: 本研究探讨了VisNet及其增强变体在物体识别和对称性分类中的性能，通过集成径向基函数神经元、马氏距离学习和视网膜预处理等方法，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 人类视觉系统在物体识别方面表现出色，但在人工系统中复制类似能力仍具挑战性。本研究旨在验证生物启发模型VisNet及其扩展在视觉识别任务中的有效性。

Method: 使用VisNet及其增强变体，结合Hebbian学习、时间连续性、径向基函数神经元、马氏距离学习和视网膜预处理，构建不变性表征。

Result: 在MNIST、CIFAR10和自定义对称物体数据集上的实验表明，增强版VisNet变体相比基线模型显著提高了识别准确率。

Conclusion: VisNet启发架构具有适应性和生物相关性，为神经科学和人工智能中的视觉识别提供了强大且可解释的框架。

Abstract: Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.
  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations

</details>


### [13] [Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency](https://arxiv.org/abs/2511.08901)
*Riling Wei,Kelu Yao,Chuanguang Yang,Jin Wang,Zhuoyan Gao,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种非对称跨模态知识蒸馏方法（ACKD），解决了现实场景中配对模态数据稀缺的问题，通过SemBridge框架在弱语义一致性下实现跨模态知识传输。


<details>
  <summary>Details</summary>
Motivation: 现实场景中配对模态数据有限，传统的对称跨模态知识蒸馏（SCKD）应用受限，需要研究在弱语义一致性下的知识蒸馏方法。

Method: 提出SemBridge框架，包含学生友好匹配模块（利用自监督学习获取语义知识并动态选择相关教师样本）和语义感知知识对齐模块（使用拉格朗日优化寻找最优传输路径）。

Result: 在遥感场景分类任务中，使用多光谱和RGB图像构建的基准数据集上，与7种现有方法在6种不同模型架构上的对比实验表明，该方法达到了最先进的性能。

Conclusion: ACKD方法在弱语义一致性下有效实现了跨模态知识蒸馏，SemBridge框架通过动态样本选择和最优传输路径优化，显著提升了知识传输效率。

Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.

</details>


### [14] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出了一个融合视觉预测与LLM结构先验的半监督文档布局理解框架，通过概率加权方法在轻量级和预训练架构上均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 文档布局理解仍然需要大量标注数据，现有半监督学习方法未能充分利用文本预训练LLM提供的结构先验信息。

Method: 使用OCR-LLM流水线推断文档层次区域，通过逆方差融合将教师检测器输出与LLM结构先验结合生成精炼伪标签，采用实例自适应门控机制。

Result: 在PubLayNet数据集上，轻量级SwiftFormer仅用5%标签达到88.2±0.3 AP；与LayoutLMv3结合达到89.7±0.4 AP，超越标准半监督学习，匹配需要1亿页多模态预训练的UDOP。

Conclusion: LLM结构先验与轻量级和预训练架构具有互补性，实例自适应门控、开源LLM隐私保护部署以及语义消歧能力是该方法的关键优势。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [15] [HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing](https://arxiv.org/abs/2511.08908)
*Shuji Ono*

Main category: cs.CV

TL;DR: HitoMi-Cam是一种轻量级、形状无关的人员检测方法，利用服装的光谱反射特性，在资源受限的边缘设备上实现实时检测，在形状不可预测的场景中优于CNN模型。


<details>
  <summary>Details</summary>
Motivation: 解决CNN目标检测的形状依赖性缺陷，特别是在训练数据中未包含的姿态下性能下降的问题，为灾难救援等形状不可预测场景提供可靠检测方案。

Method: 基于光谱反射特性的形状无关人员检测方法，在无GPU的资源受限边缘设备上实现HitoMi-Cam系统。

Result: 处理速度达到23.2fps（253x190像素），在模拟搜救场景中平均精度达93.5%，远超CNN模型的最佳精度53.8%，误报率极低。

Conclusion: HitoMi-Cam不是CNN检测器的替代品，而是在特定条件下（如形状不可预测的灾难救援场景）的补充工具，光谱基人员检测在边缘设备上具有实际可行性。

Abstract: While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.

</details>


### [16] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为负实体抑制（NES）的方法来解决零样本图像描述中的幻觉问题，通过合成图像、过滤负实体和注意力级抑制来减少跨域描述中的错误内容。


<details>
  <summary>Details</summary>
Motivation: 现有的仅文本训练方法在跨域泛化时容易产生幻觉内容，而基于检索的方法可能因检索到不相关实体而加剧幻觉问题。

Method: NES包含三个阶段：使用合成图像确保图像到文本检索的一致性；从检索内容中过滤负实体；应用注意力级抑制来减少幻觉倾向特征的影响。

Result: 在多个基准测试中，NES在保持域内性能竞争力的同时，提高了跨域迁移能力并降低了幻觉率，在零样本图像描述中达到了新的最先进结果。

Conclusion: NES方法有效解决了零样本图像描述中的幻觉问题，通过负实体抑制机制显著提升了跨域泛化性能。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [17] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: SPEED-Q是一个针对1B-2B参数规模视觉语言模型的低比特权重量化框架，通过分阶段敏感度自适应机制和蒸馏增强策略，解决了视觉与语言组件量化敏感度差异大和低比特量化训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型需要低延迟和隐私保护，但现有研究很少探索对适合边缘设备的1B-2B参数模型的激进量化方法。

Method: 提出SPEED-Q框架，包含分阶段敏感度自适应机制来协调不同模态的性能，以及蒸馏增强量化策略来稳定训练过程并减少数据依赖。

Result: 在多个基准测试中，SPEED-Q在2位设置下比现有量化方法准确率提高6倍，在2位和4位设置下均优于现有设备端视觉语言模型。

Conclusion: SPEED-Q是首个专门为小型十亿参数视觉语言模型低比特量化设计的框架，能够实现准确、稳定且数据高效的复杂视觉语言模型量化。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [18] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++是一个用于森林环境中LiDAR地点识别、重排序和6自由度度量定位的端到端框架，使用八叉树变换器提取多粒度层次局部描述符，在挑战性场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决森林环境中由于杂乱、自相似性和视角变化导致的LiDAR地点识别和定位挑战，特别是在地面到地面和地面到空中场景中。

Method: 采用八叉树变换器提取层次局部描述符，提出可学习的多尺度几何验证模块减少重排序失败，使用从粗到精的配准方法。

Result: 在CS-Wild-Places数据集上平均Recall@1达到90.7%，比基线提升29.6个百分点；在Wild-Places和MulRan数据集上分别达到91.7%和96.0%；97.2%的6-DoF配准尝试误差小于2米和5度；运行时间比RANSAC快两个数量级。

Conclusion: HOTFLoc++在具有挑战性的森林环境中实现了最先进的性能，多尺度重排序模块将定位误差平均减少约2倍，同时保持高计算效率。

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [19] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器视觉导向压缩的人机协同压缩方法Diff-FCHM，通过扩散先验恢复人类视觉的高保真细节，在机器视觉和人类视觉压缩方面均取得显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协同压缩方法主要基于人类视觉压缩流程，在整合机器视觉压缩时存在复杂度和比特率方面的不足。机器视觉仅关注图像/视频中的核心区域，所需信息远少于人类视觉压缩信息。

Method: 提出基于机器视觉导向压缩的新型协同压缩方法，开发即插即用的可变比特率策略，通过扩散先验逐步聚合机器视觉压缩的语义信息，恢复人类视觉的高保真细节。

Result: 实验结果表明，Diff-FCHM在机器视觉和人类视觉压缩方面均取得持续优异的性能，具有显著优势。

Conclusion: 该方法首次成功实现了以机器视觉为基础的人机协同压缩，为同时服务于人类感知和机器智能的图像/视频数据压缩提供了有效解决方案。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [20] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 本文提出了一种分层蒸馏（HD）框架，将轨迹蒸馏和分布蒸馏从独立范式转变为协同组件，通过轨迹蒸馏建立结构草图，为后续分布蒸馏提供近最优初始化，并结合自适应加权判别器（AWD）进行细节精炼，实现了单步扩散模型的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的推理延迟是其实时应用的关键障碍。现有的轨迹蒸馏和分布蒸馏方法存在根本性权衡：轨迹蒸馏保留全局结构但牺牲高频细节，分布蒸馏可实现更高保真度但存在模式崩溃和不稳定训练问题。

Method: 提出分层蒸馏框架：1）利用轨迹蒸馏建立结构草图，为分布蒸馏提供近最优初始化；2）引入自适应加权判别器（AWD），通过动态分配token权重专注于局部缺陷，实现高效细节精炼。

Result: 在ImageNet 256×256上，单步模型FID达到2.26，与250步教师模型相当；在高分辨率文本到图像MJHQ基准测试中也取得有希望的结果，证明了方法的通用性。

Conclusion: 该方法为高保真单步扩散模型建立了一个稳健的新范式，通过协同利用轨迹蒸馏和分布蒸馏的优势，克服了现有方法的局限性。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [21] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的双向参考协调视频压缩方法BRHVC，通过双向运动收敛和双向上下文融合技术，优化双向参考帧的利用，在HEVC数据集上超越了传统编码方法VTM-RA。


<details>
  <summary>Details</summary>
Motivation: 神经B帧视频压缩相对于P帧压缩研究不足，其分层编码可能导致连续时间预测复杂化，特别是在帧跨度较大的层次上，两个参考帧的贡献可能不平衡。

Method: 提出BRHVC方法，包含双向运动收敛(BMC)和双向上下文融合(BCF)。BMC在运动压缩中收敛多个光流，实现更准确的大规模运动补偿；BCF在运动补偿精度指导下显式建模参考上下文的权重。

Result: 实验结果表明，BRHVC在HEVC数据集上超越了之前最先进的神经视频压缩方法，甚至超过了传统编码VTM-RA（随机访问配置）。

Conclusion: 通过更高效的运动和上下文处理，BRHVC能够有效协调双向参考，显著提升了神经B帧视频压缩的性能。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [22] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于Hausdorff维度的FGM-HD框架，通过可学习的HD估计方法和HD引导的拒绝采样，在保持图像质量的同时显著提升了生成图像的多样性。


<details>
  <summary>Details</summary>
Motivation: 分形生成模型(FGM)在生成高质量图像方面很高效，但其固有的自相似性限制了输出图像的多样性。为了解决这个问题，需要一种方法来增强生成输出的多样性。

Method: 1. 提出可学习的HD估计方法，直接从图像嵌入预测HD；2. 在训练时采用基于HD的损失函数和单调动量驱动的调度策略；3. 在推理时使用HD引导的拒绝采样来选择几何更丰富的输出。

Result: 在ImageNet数据集上的实验表明，FGM-HD框架相比原始FGM实现了39%的输出多样性提升，同时保持了可比的图像质量。

Conclusion: 这是首个将HD引入FGM的工作，有效增强了生成输出的多样性，并为FGM发展提供了理论贡献。

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [23] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: AuthSig是一个基于生成模型和水印的静态电子签名框架，通过风格嵌入微调隐式编码水印比特，实现"一次签名，一次使用"策略，有效防止恶意复制和重用。


<details>
  <summary>Details</summary>
Motivation: 随着无纸化工作流程的深入，静态扫描签名因其便利性仍然普遍使用，但这些静态图像几乎失去了认证属性，无法可靠验证且易受恶意复制和重用。

Method: 利用人类视觉系统对细微风格变化不敏感的特性，在生成过程中精细调节风格嵌入来隐式编码水印比特；引入关键点驱动的数据增强策略来增强风格多样性以支持鲁棒的水印嵌入。

Result: 实验结果显示，AuthSig在数字域失真和签名特定退化下实现了超过98%的提取准确率，在打印扫描场景中仍保持有效。

Conclusion: AuthSig框架成功地将认证信息绑定到签名图像中，为静态电子签名提供了可靠的防复制和防重用保护。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [24] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出了基于核心集的双重检索框架CoDR，用于改进大视觉语言模型中的上下文学习演示选择，解决了传统方法在效率和效果上的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 上下文学习的效果严重依赖于演示样本的选择，而传统方法如随机采样、基于相似性的采样和信息得分采样往往效率低下或性能不佳，难以在效率和效果之间取得平衡。

Method: 提出了基于核心集的双重检索框架CoDR，包括：1）使用聚类剪枝方法构建多样化核心集，提高与查询的对齐度；2）开发双重检索机制，在保持效率的同时实现全局演示选择。

Result: 实验结果表明，该方法相比现有策略显著提升了上下文学习的性能，为有效且高效的演示选择提供了稳健解决方案。

Conclusion: CoDR框架通过多样化核心集和双重检索机制，成功解决了演示选择中的效率-效果平衡问题，为大视觉语言模型的上下文学习提供了更优的演示选择方案。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [25] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: 提出了一种基于小波扩散Transformer的微动脉瘤检测框架WDT-MD，解决了现有扩散模型在糖尿病视网膜病变筛查中的三个关键问题：身份映射、假阳性率高和正常特征重建不佳。


<details>
  <summary>Details</summary>
Motivation: 微动脉瘤是糖尿病视网膜病变的最早病理特征，但手动筛查劳动密集且易出错。现有基于扩散的异常检测方法存在身份映射、难以区分微动脉瘤与其他异常、正常特征重建不佳三个根本性限制，阻碍了临床应用。

Method: WDT-MD框架包含三个关键创新：噪声编码图像条件机制避免身份映射；通过修复合成伪正常模式引入像素级监督；结合扩散Transformer全局建模能力与多尺度小波分析的小波扩散Transformer架构。

Result: 在IDRiD和e-ophtha MA数据集上的综合实验表明，WDT-MD在像素级和图像级微动脉瘤检测方面均优于最先进的方法。

Conclusion: WDT-MD框架显著提升了微动脉瘤检测性能，为改善早期糖尿病视网膜病变筛查提供了重要前景。

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [26] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 提出了一种结合去噪项的变分分割模型，针对受Gamma分布乘性噪声和泊松噪声污染的图像，在ICTM框架下集成I-散度项和自适应TV正则化器，通过灰度指示器实现空间自适应权重，并估计偏置场处理强度不均匀性。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法在图像被严重噪声污染和存在强度不均匀性时性能下降，需要开发能够同时处理噪声和强度不均匀性的鲁棒分割方法。

Method: 在ICTM框架下提出变分分割模型，集成I-散度去噪项和自适应TV正则化器，使用灰度指示器生成空间自适应权重，估计平滑变化的偏置场，采用RMSAV方案进行高效优化。

Result: 在合成和真实世界图像上的广泛实验表明，该模型在处理强度不均匀性和多种噪声类型时，相比竞争方法实现了更优的准确性和鲁棒性。

Conclusion: 所提出的模型能够有效处理受Gamma分布乘性噪声和泊松噪声污染的图像，同时解决强度不均匀性问题，在分割准确性和鲁棒性方面优于现有方法。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [27] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: T-Rex-Omni是一个新颖的开放集目标检测框架，通过引入负视觉提示来否定硬负干扰物，解决了现有方法仅依赖正提示的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前开放集目标检测器仅依赖基于文本描述或视觉示例的正提示，这种仅正范式在面对视觉相似但语义不同的干扰物时存在一致脆弱性。

Method: 提出统一视觉提示编码器联合处理正负视觉提示；训练免费的否定负计算模块动态抑制负响应；否定负铰链损失通过微调增强正负嵌入间的判别性边界。

Result: 在零样本检测中表现出色，显著缩小了视觉提示与文本提示方法间的性能差距，在长尾场景中表现尤为突出（LVIS-minival上51.2 AP_r）。

Conclusion: 负提示成为推进开放集视觉识别系统的关键新维度，T-Rex-Omni支持灵活部署在仅正和联合正负推理模式中。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [28] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: Owl是一个基于因果关系的双模态注意力重加权框架，通过建模幻觉过程的结构因果图，将分解的视觉和文本注意力作为中介，有效减少大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言解码器的缓解方法通常独立调节视觉或文本注意力，忽视了它们作为两个关键因果因素的交互作用。物体幻觉是大型视觉语言模型中的一个关键挑战，模型生成与视觉输入不一致的内容。

Method: 提出VTACR（视觉到文本注意力贡献比）指标量化解码过程中的模态贡献不平衡；设计细粒度注意力干预机制，根据VTACR信号动态调整token级和layer级注意力；提出双路径对比解码策略，一条路径强调视觉基础预测，另一条放大幻觉预测。

Result: 在POPE和CHAIR基准测试上的实验结果显示，Owl实现了显著的幻觉减少，在保持视觉语言理解能力的同时，在忠实度方面创造了新的SOTA。

Conclusion: Owl框架通过因果建模和注意力重加权，有效缓解了大型视觉语言模型中的物体幻觉问题，同时保持了模型的视觉语言理解能力。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [29] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出了一种密集跨尺度图像对齐模型，通过考虑跨尺度特征相关性来降低对齐难度，支持在精度和效率之间灵活权衡，并引入全空间相关模块和恰可察觉差异来提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像对齐方法存在精度有限和计算复杂度高的问题，需要开发更准确且高效的对齐方法。

Method: 采用密集跨尺度图像对齐模型，考虑跨尺度特征相关性，通过调整尺度数量实现精度与效率的权衡，引入全空间相关模块和恰可察觉差异机制。

Result: 大量定量和定性实验表明，该方法在精度和效率方面均优于现有最先进方法。

Conclusion: 所提出的密集跨尺度图像对齐模型在保持低计算成本的同时，显著提升了图像对齐的精度和效率。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [30] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: PAN是一个通用、可交互、长视野的世界模型，通过高质量视频模拟预测未来世界状态，结合自回归潜在动态主干和视频扩散解码器，实现潜在空间推理与世界动态的统一。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏因果控制、交互性和长视野一致性，而现有世界模型往往局限于特定领域且深度和可控性有限，难以泛化到多样化环境和交互形式。

Method: 采用生成潜在预测(GLP)架构，结合基于大语言模型的自回归潜在动态主干（支持语言指定动作）和视频扩散解码器（重建感知细节和时间一致的视觉观察）。

Result: 在大规模视频-动作对数据集上训练，PAN在动作条件世界模拟、长视野预测和模拟推理方面表现优异，相比其他视频生成器和世界模型具有更强性能。

Conclusion: PAN朝着通用世界模型迈进一步，能够为推理和行动提供未来世界状态的预测模拟。

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [31] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: 本文提出了一种名为方向正交反攻击（DOC）的方法，通过引入正交梯度方向和动量更新来增强反攻击的多样性，从而提高视觉-语言预训练模型在测试时防御中的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时反攻击方法（TTC）由于对抗攻击和反攻击优化目标的基本差异，仅基于对抗输入的梯度生成反攻击，限制了搜索空间，导致反攻击过拟合有限的对抗模式，缺乏多样性来完全中和广泛的扰动。

Method: 提出方向正交反攻击（DOC），通过结合正交梯度方向和基于动量的更新来增强反攻击优化，扩展反攻击空间的探索并增加扰动的多样性。同时提出基于平均余弦相似度的方向敏感度评分，通过改进示例区分和自适应调节反攻击强度来提升DOC。

Result: 在16个数据集上的广泛实验表明，DOC在各种攻击下提高了对抗鲁棒性，同时保持了竞争力的干净准确率。

Conclusion: 增强反攻击的多样性和覆盖范围对于提高测试时防御中的对抗鲁棒性至关重要，DOC方法通过正交梯度方向和动量更新有效实现了这一目标。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [32] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出了组合增量学习（CompIL）框架，用于在组合零样本学习中持续学习新组合，通过伪回放框架和语言基元蒸馏机制来提升组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据不断涌现，组合可能无限、长尾且不完全可见，需要模型以增量方式逐步提升组合泛化能力。

Method: 提出了伪回放框架，利用视觉合成器合成已学习组合的视觉表示，并使用语言基元蒸馏机制在学习过程中保持对齐的基元表示。

Result: 在构建的MIT-States-CompIL和C-GQA-CompIL基准上进行了广泛实验，证明了所提框架的有效性。

Conclusion: 提出的CompIL框架能够有效提升模型在组合零样本学习任务中的组合泛化能力。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [33] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 本文介绍了DKDS数据集，专门针对古日文草书体Kuzushiji的退化文档和印章噪声问题，提供了文本印章检测和文档二值化两个基准任务。


<details>
  <summary>Details</summary>
Motivation: 现有OCR方法在干净的Kuzushiji文档上表现良好，但无法有效处理文档退化和印章等噪声，且缺乏专门针对这些挑战的数据集。

Method: 构建了DKDS数据集，包含退化Kuzushiji文档和印章；为文本印章检测任务提供YOLO模型基线，为文档二值化任务提供传统算法、K-means聚类和GAN方法的基线。

Result: 建立了包含退化文档和印章的DKDS数据集，并提供了两个基准任务的基线结果，数据集和实现代码已公开。

Conclusion: DKDS数据集填补了Kuzushiji识别中处理噪声挑战的数据空白，为相关研究提供了新的基准和基线方法。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [34] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: PressTrack-HMR是一个基于压力信号的多人全局人体网格恢复方法，通过检测-跟踪策略从原始压力数据中分离个体信号，并在多人交互压力数据集MIP上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在多人场景中面临遮挡、光照不足和隐私问题，而基于触觉的压力信号提供了一种无遮挡、保护隐私的替代方案，但多人同时行走时如何区分混杂的压力信号是待解决的挑战。

Method: 提出PressTrack-HMR，采用自上而下的检测-跟踪策略，首先从原始压力数据中识别和分割每个个体的压力信号，然后对每个提取的个体信号执行人体网格恢复。

Result: 实验结果表明，该方法在多人人体网格恢复方面表现出色，MPJPE为89.2mm，WA-MPJPE100为112.6mm，展示了触觉垫在多人动作识别中的潜力。

Conclusion: PressTrack-HMR证明了仅使用压力信号进行多人全局人体网格恢复的可行性，为基于压力的人类运动分析提供了新方向，并发布了MIP数据集促进进一步研究。

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [35] [DBINDS - Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: DBINDS是一种基于扩散模型反演的AI生成视频检测器，通过分析潜在空间动态而非像素级特征来识别生成视频，在跨生成器检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术快速发展，对内容安全和取证分析构成严重挑战。现有检测器主要依赖像素级视觉线索，对未见过的生成器泛化能力差。

Method: 提出DBINDS检测器，基于扩散模型反演分析潜在空间动态，发现真实视频和生成视频的初始噪声序列存在系统性差异，构建初始噪声差异序列并提取多域多尺度特征，使用特征优化和贝叶斯搜索调优的LightGBM分类器。

Result: DBINDS在GenVidBench基准测试中表现出强大的跨生成器性能，在单一生成器上训练即可实现良好的泛化能力和鲁棒性，在有限数据设置下表现优异。

Conclusion: 基于扩散模型反演的潜在空间动态分析为AI生成视频检测提供了有效的新方法，DBINDS在跨生成器检测方面具有显著优势。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [36] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一个用于评估皮肤科多模态大语言模型的新框架，包括DermBench基准和DermEval自动评估器，能够对模型生成的诊断叙述进行临床意义、可重复和可扩展的评估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型越来越多地用于直接从图像生成皮肤科诊断叙述，但可靠的评估仍是临床负责任部署的主要瓶颈。

Method: 构建DermBench基准，包含4000张真实皮肤科图像和专家认证的诊断叙述，使用基于LLM的评判器在临床维度上评分；训练DermEval无参考多模态评估器，给定图像和生成叙述，输出结构化批评和评分。

Result: 在4500个案例的多样化数据集上实验表明，DermBench和DermEval与专家评分高度一致，平均偏差分别为0.251和0.117（满分5分）。

Conclusion: 该框架为不同多模态LLM的诊断能力和可信度提供了可靠测量，支持细粒度分析和模型局限性识别。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [37] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: TACO是一个通过自验证和置信度校准来减少多模态大语言模型幻觉的框架，无需外部视觉专家，在多个基准测试中表现优于直接提示和视觉对比解码方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型经常出现幻觉问题，特别是在对象存在、属性或关系方面的错误，这影响了它们的可靠性。

Method: TACO将响应分解为原子查询，通过重新表述减少对措辞的敏感性，使用自一致性（黑盒）或自置信度（灰盒）聚合来估计置信度，然后通过语言模型优化答案。

Result: 在五个基准测试（POPE、MME、HallusionBench、AMBER和MM-Hal Bench）和两个MLLM上的实验表明，TACO持续优于直接提示和视觉对比解码，减少了系统偏差并改善了置信度校准。

Conclusion: TACO通过自验证和置信度校准有效增强了多模态大语言模型的忠实度，展示了其在减轻幻觉方面的有效性。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [38] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 本文提出RichKD框架，通过融合传统教师模型和CLIP的视觉-语言知识来增强多教师知识蒸馏，利用多提示文本指导提升知识多样性和蒸馏质量。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要依赖单模态视觉信息，缺乏知识多样性，忽视了跨模态表示的潜力。CLIP的视觉-语言知识作为补充监督源在知识蒸馏领域尚未充分探索。

Method: 提出简单有效的框架，将传统教师的logits和特征与CLIP的logits和特征进行融合，结合CLIP的多提示文本指导，捕获数据集特定和语义丰富的视觉线索。

Result: 融合后的教师产生更自信可靠的预测，显著增加自信正确案例，减少自信错误案例。CLIP融合还改善了整个logit分布，为非目标类别生成语义有意义的概率，提高了类间一致性和蒸馏质量。

Conclusion: 尽管方法简单，但RichKD在多个基准测试中持续优于现有基线，并在分布偏移和输入损坏下表现出更强的鲁棒性。

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [39] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: DensiCrafter是一个生成轻量自支撑3D空心结构的框架，通过优化密度场实现材料减重43%，保持几何保真度且无需仿真。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型忽视物理约束和可制造性，需要解决生成轻量且自支撑结构的问题。

Method: 从Trellis生成的粗体素网格出发，将其解释为连续密度场进行优化，引入三个可微分、物理约束且无需仿真的损失项，结合质量正则化和受限优化域。

Result: 在文本到3D任务中实现材料质量减少43%，相比基线方法提高稳定性并保持高几何保真度。

Conclusion: 该方法可无缝集成预训练模型，实际3D打印实验证实空心设计可可靠制造且具有自支撑能力。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [40] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: 本文提出了一种在师生框架中引入反馈机制的方法，通过学生向教师提供反馈来纠正伪标签中的错误，从而解决半监督医学图像分割中的错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 传统的师生范式在半监督医学图像分割中容易受到图像模糊性的影响，导致错误监督和自我强化偏差。现有方法通常依赖外部修改，而忽视了框架内在的错误纠正潜力。

Method: 在师生框架中引入反馈机制，包括反馈归因器（识别触发学生更新的伪标签）和反馈接收器（确定反馈应用位置）。进一步提出双教师反馈模型，通过跨教师监督解决分歧并避免一致错误。

Result: 在三个医学图像基准测试上的综合评估表明，该方法能有效解决半监督医学图像分割中的错误传播问题。

Conclusion: 通过将反馈机制整合到师生框架中，能够有效对抗错误确认，提高半监督医学图像分割的性能。

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [41] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: FQ-PETR是一个针对PETR系列3D检测模型的全量化框架，通过量化友好的位置嵌入、双查找表和非线性算子优化，在W8A8量化下实现接近浮点精度（仅1%性能下降），同时降低75%延迟。


<details>
  <summary>Details</summary>
Motivation: PETR系列模型在3D检测基准上表现出色，但面临高计算成本和内存占用的部署挑战。现有量化方法直接应用于PETR会导致严重精度下降，主要由于多模态特征幅度差异和非线性算子量化效率问题。

Method: 1. 量化友好的LiDAR射线位置嵌入：用单点采样替换多点采样，消除非线性操作；2. 双查找表：用两个级联线性查找表近似复杂非线性函数；3. 数值稳定后量化：在softmax数值稳定后进行量化，减少注意力失真。

Result: 在PETR、StreamPETR、PETRv2、MV2d等模型上，FQ-PETR在W8A8量化下实现接近浮点精度（仅1%性能下降），同时减少高达75%的延迟，显著优于现有PTQ和QAT基线方法。

Conclusion: FQ-PETR成功解决了PETR系列模型量化中的关键挑战，实现了高效部署所需的精度与效率平衡，为自动驾驶中的3D检测模型部署提供了实用解决方案。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [42] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为Flora的零样本骨架动作识别方法，通过灵活的邻居感知语义调整和开放形式的分布感知流分类器，解决了现有方法中的点对点对齐脆弱性和分类器决策边界僵化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本骨架动作识别方法存在两个基本问题：(i)由于不完美的语义导致的脆弱点对点对齐；(ii)受限于静态决策边界和粗粒度锚点的僵化分类器。

Method: Flora方法包含两个核心组件：1)灵活邻居感知语义调整，通过整合相邻类间上下文线索形成方向感知区域语义，结合跨模态几何一致性目标确保稳定的点对区域对齐；2)开放形式分布感知流分类器，使用无噪声流匹配来弥合语义和骨架潜在嵌入之间的模态分布差距，同时通过条件无关对比正则化增强可区分性。

Result: 在三个基准数据集上的广泛实验验证了该方法的有效性，即使在仅使用10%的可见数据训练时也表现出特别令人印象深刻的性能。

Conclusion: Flora方法通过创新的语义调整和流分类器设计，有效解决了零样本骨架动作识别中的关键挑战，显著提升了识别性能。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [43] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: 本文提出了OUGS框架，通过基于3D高斯原语物理参数的不确定性建模和语义分割集成，实现了针对特定物体的高效主动重建。


<details>
  <summary>Details</summary>
Motivation: 现有主动重建方法依赖场景级不确定性指标，在复杂场景中容易受到无关背景干扰，导致物体中心任务中的视图选择效率低下。

Method: 从3D高斯原语的物理参数（位置、尺度、旋转）直接推导不确定性，通过渲染雅可比矩阵传播协方差，并集成语义分割掩码生成针对性的物体感知不确定性评分。

Result: 在公共数据集上的实验表明，该方法显著提高了3DGS重建过程的效率，相比现有最先进方法在目标物体上获得了更高质量的重建结果。

Conclusion: OUGS框架不仅提供了更有效的主动视图选择策略，还能作为全局场景的鲁棒不确定性估计器，解决了物体中心重建中的关键挑战。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [44] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文提出了一个基于视觉的支气管镜导航框架，通过2D-3D配准实现术中内窥镜视图与术前CT解剖的实时定位，并创建了首个公开的合成基准数据集来标准化评估。


<details>
  <summary>Details</summary>
Motivation: 解决支气管镜术中定位的挑战，包括呼吸运动、解剖变异和CT与身体之间的变形导致的对齐误差，现有视觉方法难以跨域泛化。

Method: 提出基于视觉的位姿优化框架，使用微调的模态和域不变编码器直接计算真实内窥镜RGB帧与CT渲染深度图之间的相似性，通过可微分渲染模块迭代优化相机位姿。

Result: 在仅使用合成数据训练的情况下，模型达到平均平移误差2.65毫米和旋转误差0.19弧度，在真实患者数据上表现出强大的跨域泛化能力。

Conclusion: 该框架通过迭代视觉优化实现了鲁棒、域不变的定位，新基准数据集为基于视觉的支气管镜导航标准化进展奠定了基础。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [45] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 本文构建了首个专门用于美式足球运动员检测和跟踪的数据集，并对多种检测和跟踪方法进行了比较评估，发现在拥挤场景下也能实现准确的检测和跟踪。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪方法主要关注日常场景或特定运动（如足球、篮球），而美式足球由于频繁遮挡和身体接触等固有挑战，缺乏标准化数据集，难以公平比较不同方法。

Method: 构建首个专门的美式足球运动员检测和跟踪数据集，对多种检测和跟踪方法进行对比评估，包括微调检测模型和集成重识别模型到跟踪系统中。

Result: 微调检测模型相比预训练模型性能有所提升；当这些微调检测器和重识别模型集成到跟踪系统中时，跟踪精度相比现有方法有显著改善。

Conclusion: 这项工作能够在传统方法难以处理的挑战性高密度场景中实现美式足球运动员的鲁棒检测和跟踪。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [46] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: 提出了一种双教师知识蒸馏框架，利用异构ViT教师和同构CNN教师协同指导轻量级CNN学生，通过差异感知教师加权和结构差异感知蒸馏策略，在视频动作识别任务中显著提升轻量级CNN的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在视频动作识别中表现优异但计算成本高，轻量级CNNs效率高但精度不足。现有跨架构知识蒸馏方法存在架构不匹配问题，且忽视了同构CNN教师的价值。

Method: 双教师知识蒸馏框架：1) 差异感知教师加权 - 基于教师置信度和与学生预测差异动态融合ViT和CNN教师的预测；2) 结构差异感知蒸馏 - 学生通过轻量级辅助分支学习ViT和CNN教师之间的残差特征，关注可转移的架构差异。

Result: 在HMDB51、EPIC-KITCHENS-100和Kinetics-400基准测试中，方法持续优于最先进的蒸馏方法，在HMDB51上最高获得5.95%的准确率提升。

Conclusion: 双教师知识蒸馏框架有效解决了架构不匹配问题，通过协同利用异构和同构教师的知识，显著提升了轻量级CNN在视频动作识别中的性能。

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


### [47] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: DreamPose3D是一个基于扩散模型的3D人体姿态估计框架，通过动作感知推理和时间想象力，结合运动意图和关节关系建模，在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法主要依赖几何线索并独立预测每帧姿态，无法解决模糊运动问题且泛化能力有限。受人类理解和预测运动方式的启发，需要结合高层意图推理和时间一致性建模。

Method: 提出DreamPose3D框架：1) 使用从2D姿态序列提取的任务相关动作提示动态调节去噪过程；2) 引入表示编码器，将运动学关节亲和度融入注意力机制；3) 幻觉姿态解码器在训练中预测时间一致的3D姿态序列。

Result: 在Human3.6M和MPI-3DHP数据集上的广泛实验显示在所有指标上达到最先进性能。在广播棒球数据集上测试也表现出强鲁棒性，能有效处理模糊和噪声2D输入，保持时间一致性并处理意图驱动的运动变化。

Conclusion: DreamPose3D通过结合动作感知推理、关节关系建模和时间想象力，显著提升了3D人体姿态估计的准确性和鲁棒性，特别是在处理模糊运动和真实场景方面表现出色。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [48] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 本文改进了图稀疏化算法MAC，通过开发专用求解器、优化步长策略和自动连通性保证方案，使其更适合实时估计应用。


<details>
  <summary>Details</summary>
Motivation: 当前图估计方法在大规模长期图上扩展性差，MAC算法虽然能保持估计性能但计算成本高且需要手动指定边集，限制了其实时应用。

Method: 开发代数连通性专用求解器（平均2倍加速），研究MAC优化过程的步长策略，提出自动保证图连通性的方案。

Result: 这些改进使MAC在可扩展性、可靠性和实时性方面得到显著提升。

Conclusion: 通过三方面的互补贡献，MAC算法变得更加适合实时估计应用，解决了原有算法的计算瓶颈和手动配置问题。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [49] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文综述了人机协作中实现直觉信息交换和技能转移的关键组件，包括多模态输入翻译、自适应规划、角色分配、控制层和反馈机制，旨在建立持续的信息流以实现协同人机协作。


<details>
  <summary>Details</summary>
Motivation: 当前机器人和AI虽然具备强大能力，但人类往往只是被动观察者，而机器人在人类环境中无法充分发挥潜力，需要有效建模人类状态和意图并调整行为以实现协同合作。

Method: 通过建立人机通信桥梁，将多模态输入翻译为机器人可理解的表示，结合自适应规划和角色分配，通过控制层和反馈机制形成闭环交互管道。

Result: 识别并连接了实现直觉信息交换和技能转移的关键组件，建立了完整的人机交互管道框架。

Conclusion: 提出了实现更自适应、可访问的人机协作的趋势和前景方向，强调建立持续信息流对于实现协同人机协作的重要性。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [50] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: ATOM-CBF是一个新颖的安全控制框架，通过显式计算和适应分布外测量带来的认知不确定性，无需真实标签或分布偏移信息，确保具有学习感知模块的系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统的安全性具有挑战性，特别是当它们依赖学习感知模块从高维传感器数据推断系统状态时。这些感知模块容易受到认知不确定性的影响，在遇到训练期间未见过的分布外测量时经常失败。

Method: ATOM-CBF框架包含两个关键组件：(1) 分布外感知的自适应感知误差边界；(2) 集成这种自适应误差边界的安全过滤器，使过滤器能够实时调整其保守性。

Result: 在仿真中进行了实证验证，证明ATOM-CBF能够为配备LiDAR扫描的F1Tenth车辆和配备RGB图像的四足机器人保持安全性。

Conclusion: ATOM-CBF通过显式处理分布外测量带来的认知不确定性，为依赖学习感知模块的系统提供了一种有效的安全控制解决方案。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [51] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: CENIC是一种新的连续时间积分器，结合了凸时间步进和误差控制积分的最新进展，能够以接近离散时间机器人模拟器的实时速度运行，同时提供精度和收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模拟器在离散时间下运行，需要选择时间步长，这既关键又具有挑战性：大步长会产生非物理伪影，而小步长会迫使模拟运行缓慢。连续时间误差控制积分通过自动调整时间步长来达到所需精度，但现有方法难以处理接触的刚性动力学，无法满足现代机器人工作流程的速度和可扩展性要求。

Method: CENIC结合了凸时间步进和误差控制积分的最新进展，继承了连续积分和离散时间步进的优点。

Result: CENIC能够以与MuJoCo、Drake和Isaac Sim等离散时间机器人模拟器相当的快速实时速率运行，同时提供精度和收敛性保证。

Conclusion: CENIC成功解决了现有连续时间积分器在处理接触刚性动力学时的困难，满足了现代机器人工作流程对速度和可扩展性的要求。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [52] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 提出了一种针对高自由度双臂机器人的实时运动规划方法，通过利用共享关节结构构建动态路标图，有效缓解维度灾难问题，在真实超市环境中实现了0.4秒平均规划时间和99.9%的成功率。


<details>
  <summary>Details</summary>
Motivation: 高自由度双臂机器人在未知动态环境中的实时运动规划面临维度灾难和复杂避障约束的挑战，需要新的方法来提高规划效率。

Method: 为每个运动链（左臂+躯干、右臂+躯干）构建具有共享关节特定结构的动态路标图，利用这种结构高效搜索两个路标图的组合，从而规避维度灾难。

Result: 在真实超市环境中使用19自由度移动操作机器人执行杂货配送任务，实现了0.4秒平均规划时间，超过2000次运动规划的成功率达到99.9%。

Conclusion: 通过利用双臂机器人共享关节的结构特性，可以有效缓解高维配置空间带来的维度灾难问题，实现实时运动规划。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [53] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: CoUGARs是一个低成本、可配置的自主水下机器人平台，用于多智能体自主研究，基础设计成本低于3000美元，支持声学协作定位研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多智能体自主水下测试面临巨大的财务和工程挑战，需要低成本、可配置的解决方案。

Method: 开发基于商用和3D打印部件的低成本AUV平台，配备DVL和USBL声学阵列，使用容器化软件栈并与HoloOcean模拟器集成。

Result: 系统在模拟和犹他州湖泊水库的实地试验中进行了测试验证。

Conclusion: CoUGARs为多智能体自主研究提供了一个经济高效、可配置的水下机器人平台解决方案。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [54] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 本文提出了一种专门针对X波段雷达的场所识别算法，通过基于目标密度的候选选择规则和故意降低雷达检测质量来实现稳健的检索性能，用于海上自主导航。


<details>
  <summary>Details</summary>
Motivation: X波段雷达作为海上船舶的主要传感器，但由于传感器分辨率低和信息内容不足，在自主导航中的应用受到限制。

Method: 提出专门针对X波段雷达的场所识别算法，包含基于目标密度的候选选择规则和故意降低雷达检测质量的方法。

Result: 在公共海上雷达数据集和自收集数据集上评估算法性能，并与最先进的雷达场所识别方法进行比较，进行了消融研究评估关键参数的敏感性。

Conclusion: 该算法能够实现仅使用X波段雷达的海上自主导航，通过特定设计的方法解决了传感器分辨率低和信息内容不足的问题。

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [55] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 提出基于PICO的机器人远程操作框架，实现低成本实时手部运动姿态数据采集，兼容RealMirror生态系统，支持在Isaac仿真环境中进行稳定精确的机器人轨迹记录，并支持多种末端执行器的实时遥操作。


<details>
  <summary>Details</summary>
Motivation: 降低上肢机器人操作研究的技术门槛，加速视觉-语言-动作(VLA)相关研究的进展。

Method: 开发PICO-based机器人远程操作框架，实现低成本实时手部运动姿态数据采集，与RealMirror生态系统原生兼容，在Isaac仿真环境中进行机器人轨迹记录。

Result: 该框架在成本效益方面优于主流视觉跟踪和运动捕捉解决方案，支持稳定精确的机器人轨迹记录，便于构建VLA数据集，并支持多种末端执行器的实时遥操作。

Conclusion: 该工作为机器人操作研究提供了低成本高效的解决方案，有助于推动VLA相关研究的发展。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [56] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出了一种具有规划级意图预测的移动机器人共享控制框架，通过意图域表示未来运动意图，结合深度强化学习解决意图预测和路径重规划问题，显著降低了操作员工作负荷并提高了安全性。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人共享控制中，有效理解人类运动意图对于实现无缝人机协作至关重要，现有方法在这方面存在局限性。

Method: 设计了路径重规划算法，引入意图域概念表示未来运动意图，将意图域预测和路径重规划问题建模为马尔可夫决策过程并通过深度强化学习求解，开发了基于Voronoi图的人类轨迹生成算法实现无人类参与的仿真训练。

Result: 大量仿真和真实用户研究表明，该方法显著降低了操作员工作负荷，提高了安全性，同时与现有辅助遥操作方法相比不损害任务效率。

Conclusion: 提出的共享控制框架通过规划级意图预测和深度强化学习方法，有效提升了移动机器人共享控制的性能和用户体验。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [57] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: SCOPE是一个零样本视觉导航框架，通过显式利用边界信息驱动基于潜力的探索，结合视觉语言模型估计探索潜力并构建时空潜力图，还包含自我重新考虑机制来提升决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本研究虽然通过记忆机制改善了长时程规划，但忽视了视觉边界对轨迹和观测的根本影响，且难以推断部分视觉观察与导航目标之间的关系。

Method: 提出SCOPE框架：1）利用视觉语言模型估计探索潜力；2）构建时空潜力图捕捉边界动态；3）引入自我重新考虑机制重新评估和优化先前决策。

Result: 在两个不同的具身导航任务上，SCOPE在准确率上比最先进基线高出4.6%，其核心组件带来了更好的校准、更强的泛化能力和更高的决策质量。

Conclusion: SCOPE通过显式利用边界信息和自我重新考虑机制，在零样本视觉导航中实现了更明智和目标相关的决策，显著提升了导航性能。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [58] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 该论文提出了一种新框架，将视觉语言模型从被动观察者转变为主动策略制定者，通过结构化思维链提示、动态动作历史集成和障碍物地图解读等技术，显著提升了机器人导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用视觉语言模型的推理能力，因此需要将其角色从被动观察者转变为导航过程中的主动策略制定者，以释放其在机器人领域的全部潜力。

Method: 采用三重技术：结构化思维链提示激发逻辑推理；动态集成智能体近期动作历史防止循环；VLM能够解读俯视障碍物地图和第一人称视图以增强空间感知。

Result: 在HM3D、Gibson和MP3D等挑战性基准测试中，该方法产生了极其直接和逻辑的轨迹，导航效率相比现有方法有显著提升。

Conclusion: 该方法为开发更强大的具身智能体开辟了新路径，通过将VLM作为高级规划器来指导基于前沿的探索智能体，实现了导航性能的实质性改进。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [59] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: UniMM-V2X是一个端到端多智能体框架，通过多层次融合策略实现感知、预测和规划的层次化协作，采用混合专家(MoE)架构动态增强BEV表示，在DAIR-V2X数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶存在感知受限和孤立决策的问题，现有多智能体方法主要关注感知层面，忽视了与下游规划和控制的协调，未能充分利用端到端自动驾驶的潜力。

Method: 提出多层次融合策略统一感知和预测协作，让智能体共享查询并协同推理；采用MoE架构动态增强BEV表示，并将MoE扩展到解码器以更好捕捉多样运动模式。

Result: 在DAIR-V2X数据集上，感知精度提升39.7%，预测误差降低7.2%，规划性能提升33.2%，相比UniV2X显著改进。

Conclusion: MoE增强的多层次协作范式展示了强大的性能，为端到端多智能体自动驾驶提供了有效解决方案。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [60] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: 本文提出了一种增强型多策略矮獴优化算法(EDMO)，用于解决无人机三维路径规划问题。该算法整合了三种新策略来应对传统元启发式算法在复杂场景中的早熟收敛和缺乏解多样性等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛应用，有效的路径规划变得日益重要。虽然传统搜索方法已被广泛应用，但元启发式算法因其效率和问题特定启发式而受到欢迎。然而，在复杂场景中，早熟收敛和缺乏解多样性等问题仍然阻碍其性能。

Method: EDMO算法集成了三种新策略：(1)动态量子隧道优化策略(DQTOS)，使粒子能够概率性地逃离局部最优；(2)生物趋光动态聚焦搜索策略(BDFSS)，受微生物趋光性启发，用于自适应局部细化；(3)正交透镜对立学习策略(OLOBL)，通过结构化维度重组增强全局探索。

Result: EDMO在CEC2017和CEC2020的39个标准测试函数上进行了基准测试，在收敛速度、鲁棒性和优化精度方面优于14种先进算法。在无人机三维路径规划和三个工程设计任务上的实际验证证实了其在需要智能、自适应和时间高效规划的现场机器人任务中的实际适用性和有效性。

Conclusion: EDMO算法通过整合三种创新策略，有效解决了元启发式算法在复杂无人机路径规划中的性能瓶颈，在理论和实际应用中均表现出优越性能，为现场机器人任务提供了智能、自适应且时间高效的规划解决方案。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [61] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: 提出D-AWSIM分布式模拟器，用于大规模城市交通场景和传感器部署的仿真，支持动态地图生成，相比单机设置显著提升车辆数量和LiDAR传感器处理的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现实世界实验成本高且面临监管挑战，传统单主机模拟器无法处理大规模城市交通场景，需要分布式解决方案来支持密集交通环境和广泛传感器部署的仿真。

Method: 开发D-AWSIM分布式模拟器，将工作负载分配到多台机器上，支持大规模传感器部署和密集交通环境仿真，并集成动态地图生成框架。

Result: 评估显示D-AWSIM在车辆数量和LiDAR传感器处理方面的吞吐量相比单机设置有显著提升，并与Autoware集成验证了其在自动驾驶研究中的适用性。

Conclusion: D-AWSIM为研究人员提供了无需依赖物理测试平台的信息共享策略探索能力，是支持大规模自动驾驶系统研究的有力工具。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [62] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: APEX是一种即插即用的强化学习方法，通过整合动作先验来消除部署时对参考数据的依赖，提高样本效率并减少参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有运动跟踪方法需要大量调优并在部署时依赖参考数据，限制了适应性。

Method: 结合衰减动作先验和多批评器框架，将专家演示直接整合到强化学习中，初始偏向专家演示但逐渐允许独立探索。

Result: 在仿真和Unitree Go2机器人上的实验验证了方法的有效性，能够学习多样化运动并在不同地形和速度间迁移风格。

Conclusion: APEX通过演示引导强化学习探索，使腿式机器人能够以更高的稳定性、效率和泛化能力学习，为广泛机器人任务的自然技能获取铺平道路。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [63] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出了一个统一框架，用于实时独立控制软执行器的扭矩和刚度，通过级联控制器和分析逆动力学维持解耦控制，在动态接触过程中保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有软执行器控制器难以在动态接触瞬变中维持独立的扭矩-刚度控制，限制了其在安全人机交互中的应用。

Method: 使用统一力律捕捉不同软执行器物理特性，采用级联控制器和基于共收缩/偏置坐标的分析逆动力学方法。

Result: 仿真验证显示：软表面200倍更快稳定，刚性表面81%力减少，稳定交互对比固定策略的22-54%稳定性。

Conclusion: 该框架为肌肉骨骼拮抗系统实现自适应阻抗控制提供了基础，支持安全的人机交互。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [64] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了两个数据驱动的工具来评估具身智能数据集：多样性熵（衡量数据集信息量）和可学习性量化算法（无需训练即可评估数据集易学性），解决了现有方法在评估多模态数据集多样性和可学习性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能数据集评估方法存在局限性：多样性评估通常只统计任务和场景数量或评估孤立模态，无法全面反映数据集多样性；可学习性评估依赖耗时的模型训练，缺乏解释性且无法为数据集改进提供指导。

Method: 1. 为每个数据样本构建统一的多模态表示，基于此提出多样性熵来衡量数据集信息量；2. 引入首个可解释的数据驱动算法，无需训练即可量化数据集可学习性。

Result: 在模拟和真实世界具身数据集上的验证表明，该算法能够提供可靠、可操作的见解，使研究人员能够同时改进数据集的多样性和可学习性。

Conclusion: 这项工作为设计更高质量的具身智能数据集奠定了基础，有望推动具身智能的发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [65] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: LODESTAR是一种新颖的LiDAR-惯性里程计方法，通过退化解感知的自适应Schmidt-Kalman滤波器和退化解感知数据利用两个关键模块，解决在退化环境（如长走廊、高空飞行）中LiDAR测量不平衡或稀疏导致的病态状态估计问题。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR-惯性里程计在退化环境中性能下降，因为LiDAR测量不平衡或稀疏会导致状态估计病态。需要开发能够处理这些退化情况的方法。

Method: 提出LODESTAR方法，包含两个核心模块：DA-ASKF（退化解感知自适应Schmidt-Kalman滤波器）使用滑动窗口利用过去状态和测量作为额外约束，通过退化解感知滑动模式自适应分类状态；DA-DE（退化解感知数据利用）基于局部化贡献和雅可比矩阵条件数，修剪非信息性测量并选择性利用固定状态的测量。

Result: 实验结果表明，LODESTAR在各种退化条件下，在准确性和鲁棒性方面优于现有的基于LiDAR的里程计方法和退化解感知模块。

Conclusion: LODESTAR通过退化解感知约束优化解决了测量稀疏性问题，通过退化解感知数据利用解决了测量不平衡问题，显著提高了LiDAR-惯性里程计在退化环境中的性能。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [66] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: Humanoid-Union是一个通过自动化流程生成的大规模人形机器人运动数据集，包含260小时多样化高质量运动数据，基于人类运动视频生成。SCHUR是一个可扩展学习框架，利用该数据集探索大规模数据对人形机器人高级控制的影响。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中数据扩展的关键瓶颈，利用丰富的人类视频和运动数据作为免费的大规模数据源，通过语义相关的运动实现模态对齐和高级机器人控制学习。

Method: 提出Humanoid-Union数据集生成自动化流程，以及基于该数据集的SCHUR可扩展学习框架，探索大规模数据对高级控制的影响。

Result: SCHUR在数据和模型扩展下实现了高机器人运动生成质量和强文本-运动对齐，MPJPE重建改进37%，FID对齐改进25%，并在真实人形机器人上验证了有效性。

Conclusion: Humanoid-Union数据集和SCHUR框架为机器人学习提供了有效的大规模数据解决方案，显著提升了运动生成质量和语义对齐能力。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [67] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: UMIGen是一个统一框架，包含Cloud-UMI手持数据采集设备和可见性感知优化机制，能够高效生成与真实自我中心3D观测对齐的数据，支持跨机器人具身的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动机器人学习面临的数据收集难题：稳健策略需要大规模高质量演示数据，但当前方法收集成本高、依赖专用硬件且空间泛化能力有限。

Method: 提出UMIGen框架：1) Cloud-UMI手持设备，无需视觉SLAM同时记录点云观测-动作对；2) 可见性感知优化机制，扩展DemoGen管道至自我中心3D观测，仅生成相机视野内的点。

Result: 在模拟和真实环境中的实验表明，UMIGen支持强大的跨具身泛化能力，并在多样化操作任务中加速数据收集。

Conclusion: UMIGen能够高效生成与真实自我中心观测对齐的数据，可直接在不同机器人具身间转移而无需后处理，为机器人学习提供了有效的解决方案。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [68] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: SPIDER是一个基于物理的重新定位框架，可将人类运动数据转换为机器人可执行的动态可行轨迹，解决人机数据差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器人灵巧控制所需的大规模演示数据稀缺问题，利用丰富的人类运动数据来弥补机器人特定数据的不足。

Method: 使用基于物理的大规模采样和课程式虚拟接触指导，将仅包含运动学信息的人类演示转换为动态可行的机器人轨迹。

Result: 在9种人形/灵巧手实体和6个数据集上验证，成功率比标准采样提高18%，比强化学习基线快10倍，生成了240万帧动态可行机器人数据集。

Conclusion: SPIDER作为通用物理重定位方法，能够处理多样化质量的数据并生成高质量数据，有效支持强化学习等策略学习方法。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [69] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: WMPO是一个基于世界模型的视觉-语言-动作强化学习框架，通过像素级预测实现无需真实环境交互的在线策略优化，显著提高样本效率并实现自我纠正等涌现行为。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖专家演示，无法从失败中学习；而强化学习样本效率低。需要结合两者优势，实现高效的自学习能力。

Method: 提出WMPO框架，使用像素级预测的世界模型，将"想象"轨迹与预训练的VLA特征对齐，实现在线策略GRPO优化。

Result: 在仿真和真实机器人实验中，WMPO显著提高样本效率，获得更强性能，涌现自我纠正行为，并展示鲁棒泛化和终身学习能力。

Conclusion: WMPO为VLA模型提供了一种无需真实环境交互的高效强化学习框架，解决了样本效率问题并实现了自我改进能力。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [70] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出MAP-VLA框架，通过记忆增强提示机制提升预训练视觉-语言-动作模型在长时程机器人操作任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在长时程任务中表现不佳，主要原因是缺乏记忆机制且仅依赖即时感官输入

Method: 构建基于历史演示的记忆库，通过轨迹相似度匹配检索相关记忆，并将其作为可学习的软提示动态集成到冻结的VLA模型中

Result: 在仿真基准测试中实现7.0%的绝对性能提升，在真实机器人评估中实现25.0%的性能提升，超越当前最先进方法

Conclusion: MAP-VLA为预训练VLA模型提供了一种轻量级、即插即用的记忆增强解决方案，显著提升了长时程机器人操作任务的性能

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>
