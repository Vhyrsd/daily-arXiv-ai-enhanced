<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出一种从单目图像恢复真实尺度3D重建的方法，用于精确饮食评估，相比现有技术减少近30%体积估计误差


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如肥胖和糖尿病需要精确监测食物摄入量，但现有AI饮食评估方法难以从单目图像准确恢复食物尺寸信息，限制了其在精准营养领域的应用

Method: 利用在大规模数据集上训练的模型提取丰富视觉特征，估计重建对象的真实尺度，将单视图3D重建转换为具有物理意义的真实尺度模型

Result: 在两个公开数据集上的实验表明，该方法持续优于现有技术，平均绝对体积估计误差减少近30%

Conclusion: 该方法成功连接了3D计算机视觉与数字健康领域，展示了在精准营养领域应用的潜力

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa提出了一种新颖的显著性感知前景-背景解耦框架，通过显式引入显著性线索来解决开放词汇语义分割中CLIP等视觉语言模型的前景偏见和空间定位限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP等视觉语言模型的开放词汇语义分割方法存在两个关键限制：1）前景偏见，倾向于忽略背景区域；2）有限的空间定位能力，导致物体边界模糊。这些模型在图像-文本对预训练时偏向于显著的物体中心区域。

Method: 提出DiSa框架，包含两个核心模块：1）显著性感知解耦模块（SDM），通过显式引入显著性线索，以分而治之的方式分别建模前景和背景集成特征；2）分层细化模块（HRM），利用像素级空间上下文，通过多级更新实现通道级特征细化。

Result: 在六个基准测试上的广泛实验表明，DiSa在开放词汇语义分割任务上始终优于最先进的方法。

Conclusion: DiSa通过显著性感知的前景-背景解耦框架有效解决了现有视觉语言模型在开放词汇语义分割中的前景偏见和空间定位限制问题，显著提升了分割性能。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 提出Sparse CLIP方法，在CLIP训练中直接集成稀疏性，实现了可解释性和性能的协同优化，挑战了传统认为可解释性需要牺牲准确性的观念。


<details>
  <summary>Details</summary>
Motivation: CLIP作为视觉-语言表示学习的基石，其密集不透明的潜在表示存在显著的可解释性挑战。传统观点认为可解释性和性能存在矛盾，而现有的后处理方法（如稀疏自编码器）往往导致下游性能下降和跨模态能力丧失。

Method: 提出Sparse CLIP方法，在CLIP训练过程中直接集成稀疏性约束，而不是采用后处理方式。这种方法产生既稀疏又可解释的表示，同时保持CLIP的跨模态能力。

Result: 与稀疏自编码器相比，Sparse CLIP表示保持了下游任务的强大性能，实现了更优的可解释性，并保留了跨模态能力。稀疏特征支持直接的语义概念对齐，揭示了跨模态知识出现的训练动态。

Conclusion: 研究挑战了传统认为可解释性需要牺牲准确性的观念，证明可解释性和性能可以协同优化。这为未来模型设计提供了有前景的原则，稀疏CLIP表示支持可解释的视觉引导能力。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [4] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 该研究聚焦于H&E染色图像中细胞核实例分割的数据集标准化与评估，而非开发新算法。通过整合多个公开数据集、创建统一格式、评估数据集性能并构建融合数据集，为细胞核分割提供了新的基准。


<details>
  <summary>Details</summary>
Motivation: 当前细胞核实例分割研究主要关注算法开发，并在有限数量的公开数据集上进行基准测试。缺乏对数据集本身的系统性评估和标准化，这限制了不同算法间的公平比较和模型性能的进一步提升。

Method: 1. 通过文献综述识别手动标注的公开H&E染色图像数据集；2. 将数据集标准化为统一的输入和标注格式；3. 使用两种最先进的分割模型（CNN和CNN-ViT混合架构）系统评估和排名数据集性能；4. 提出统一的测试集（NucFuse-test）用于公平的跨数据集评估；5. 创建统一的训练集（NucFuse-train）通过合并多个数据集的图像来提升分割性能。

Result: 1. 完成了多个公开数据集的标准化和整合；2. 基于分割性能对数据集进行了系统评估和排名；3. 构建了用于公平评估的NucFuse-test测试集；4. 创建了用于提升性能的NucFuse-train训练集；5. 提供了新的细胞核实例分割基准，包括代码公开和外部验证。

Conclusion: 该研究通过数据集标准化、系统性评估和融合数据集构建，为H&E染色图像中的细胞核实例分割提供了新的训练、测试和评估基准，有助于促进该领域研究的公平比较和性能提升。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [5] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 提出SAP训练无关剪枝方法，通过识别中间层关键视觉补丁实现90%以上索引向量压缩，同时保持检索性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在细粒度视觉文档检索中索引向量开销过大，训练无关剪枝方法在高压缩场景下性能不佳，传统方法认为视觉标记重要性是查询相关的，质疑训练无关剪枝的可行性

Method: 提出结构锚点剪枝(SAP)方法，从中间层识别关键视觉补丁；引入Oracle分数保留(OSR)协议评估层间信息对压缩效率的影响

Result: 在ViDoRe基准测试中，SAP将索引向量减少超过90%同时保持稳健的检索保真度；OSR分析显示语义结构锚点补丁在中间层持续存在

Conclusion: SAP为视觉RAG提供了高度可扩展的解决方案，证明中间层包含持久的结构锚点信息，不同于传统聚焦于最终层的方法

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [6] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 提出针对扩散式大语言视觉模型LLaDA-V的结构化token剪枝策略，通过分析注意力机制发现其跨模态信息主要在中后期层聚合，因此在第一个去噪步骤的中后期层选择性剪除视觉token，减少65%计算成本的同时保持95%任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言视觉模型如LLaDA-V存在显著计算开销问题，其双向注意力机制和迭代去噪范式导致视觉token在所有层和去噪步骤中重复处理。通过注意力分析发现，与自回归解码器不同，LLaDA-V的跨模态信息主要在中后期层聚合，导致语义对齐延迟。

Method: 提出结构化token剪枝策略，受FastV启发但针对扩散模型特点进行改进：1）在第一个去噪步骤的中后期层进行剪枝，与LLaDA-V的延迟注意力聚合对齐以保持输出质量；2）第一步的剪枝策略可减少所有后续步骤的计算开销；3）选择性移除一定比例的视觉token以减少FLOPs同时保留关键语义信息。

Result: 在多个基准测试中，最佳配置将计算成本降低高达65%，同时保持平均95%的任务性能。这是首个研究扩散式大语言视觉模型中结构化token剪枝的工作。

Conclusion: 该框架为高效LLaDA-V推理提供了经验基础，并凸显了视觉感知剪枝在扩散式多模态模型中的潜力。通过针对模型特定注意力模式设计的剪枝策略，能够在显著降低计算开销的同时保持模型性能。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [7] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle是一个基于Qwen-Image-Edit构建的轻量级图像和视频风格迁移模型，通过课程持续学习框架在混合数据集上训练，实现了内容保持和风格定制的平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiTs）在内容保持风格迁移方面面临挑战，因为其内部表示中内容和风格特征存在固有纠缠。需要一种既能保持内容精确性又能实现风格定制的方法。

Method: 1. 基于Qwen-Image-Edit构建TeleStyle模型；2. 构建高质量数据集，包含特定风格和数千种多样风格类别的合成三元组；3. 引入课程持续学习框架，在干净（策划）和嘈杂（合成）的混合数据集上训练；4. 添加视频到视频风格化模块以增强时间一致性和视觉质量。

Result: TeleStyle在三个核心评估指标上达到最先进性能：风格相似性、内容一致性和美学质量。模型能够泛化到未见过的风格而不损害内容保真度。

Conclusion: TeleStyle通过创新的课程持续学习框架和混合数据集训练，成功解决了扩散变换器中内容和风格特征纠缠的问题，实现了高效的内容保持风格迁移，适用于图像和视频应用。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [8] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO：一种基于密集奖励的文本到图像生成对齐框架，通过评估每个去噪步骤的细粒度贡献来解决稀疏奖励问题，并提出奖励感知的探索空间校准方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO和流匹配模型的方法在文本到图像生成的人类偏好对齐方面取得了显著进展，但仍存在稀疏奖励问题：整个去噪轨迹的终端奖励被应用于所有中间步骤，导致全局反馈信号与中间去噪步骤的具体细粒度贡献不匹配。

Method: 提出DenseGRPO框架，包含两个关键组件：1）通过基于ODE的方法在中间清晰图像上应用奖励模型，预测每个去噪步骤的逐步奖励增益作为密集奖励；2）基于估计的密集奖励，提出奖励感知方案，通过自适应调整SDE采样器中特定时间步的随机性注入来校准探索空间。

Result: 在多个标准基准测试上的广泛实验证明了DenseGRPO的有效性，并突出了有效密集奖励在流匹配模型对齐中的关键作用。

Conclusion: DenseGRPO通过密集奖励解决了现有GRPO方法中的稀疏奖励问题，实现了更精细的人类偏好对齐，并通过奖励感知的探索空间校准提高了训练效果。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [9] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出FPL方法，通过特征投影学习将分类问题转化为特征投影问题，结合投影模型和原始CLIP预测，在有限监督下高效适应下游任务


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应VLP模型（如CLIP）到下游任务时存在性能有限、可学习参数过多或训练时间过长的问题，需要更高效有效的适应方法

Method: 提出特征投影学习（FPL）方法，构建投影模型将类别原型特征投影到查询图像特征空间并重建查询图像特征图，将负平均平方重建误差作为类别分数，将分类问题转化为特征投影问题

Result: FPL在准确率上显著超越当前最先进方法，实现了优越的性能表现

Conclusion: FPL是一种简单、高效且有效的方法，能够解决现有方法在适应CLIP模型到下游任务时的各种限制，通过特征投影学习实现了更好的性能

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [10] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出RED模型，一种可逆高效扩散模型，用于多模态图像融合，在保持扩散模型强大生成能力的同时避免分布估计问题


<details>
  <summary>Details</summary>
Motivation: 多模态图像融合需要将不同源图像的互补信息整合到统一表示中，但传统扩散模型在图像融合任务中存在细节丢失问题，这是由于马尔可夫过程中的噪声误差累积导致的。同时，在扩散模型中引入显式监督会带来计算效率挑战。

Method: 提出可逆高效扩散（RED）模型，这是一个显式监督的训练框架，继承了扩散模型的强大生成能力，同时避免了分布估计问题。该方法通过可逆设计提高计算效率。

Result: 未在摘要中明确说明具体实验结果，但暗示该方法能够解决传统扩散模型在图像融合中的细节丢失问题，同时提高计算效率。

Conclusion: RED模型为多模态图像融合提供了一种有效的解决方案，在保持扩散模型生成能力的同时，克服了传统方法中的细节丢失和计算效率问题。

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [11] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 提出LVLMs-Saliency框架，通过融合注意力权重和输入梯度来检测大视觉语言模型中的幻觉，并提出两种推理时缓解方法：基于显著性的拒绝采样和局部一致性增强。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力动态的方法仅依赖前向传播的注意力模式，忽略了梯度信号，难以可靠区分幻觉和事实性输出。需要更全面的诊断框架来检测和缓解大视觉语言模型中的幻觉问题。

Method: 提出LVLMs-Saliency梯度感知诊断框架，融合注意力权重和输入梯度量化输出token的视觉基础强度。基于分析发现，提出两种推理时方法：1) 显著性引导拒绝采样(SGRS)，在自回归解码中动态过滤低显著性候选token；2) 局部一致性增强(LocoRE)，轻量级插件模块增强当前token对最近前驱token的注意力。

Result: 在多个大视觉语言模型上的广泛实验表明，该方法显著降低了幻觉率，同时保持了流畅性和任务性能，提供了增强模型可靠性的鲁棒且可解释的解决方案。

Conclusion: LVLMs-Saliency框架通过融合梯度信号提供了更全面的幻觉检测，提出的推理时缓解方法能有效减少幻觉，为大视觉语言模型的可靠性增强提供了新途径。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [12] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出了一种新颖的无源域自适应方法，通过多视图增强和潜在空间一致性技术直接从目标域学习域不变特征，无需源域数据或复杂的伪标签技术。


<details>
  <summary>Details</summary>
Motivation: 现有域自适应方法通常需要访问源域数据、对抗训练或复杂的伪标签技术，计算成本高。为了解决这些挑战，本文旨在开发一种无需源域数据、更高效的自适应方法。

Method: 采用多视图增强和潜在空间一致性技术，直接从目标域学习域不变特征。通过生成目标域数据的多个增强视图，并在潜在空间中最小化其特征表示之间的距离来确保特征一致性。使用ConvNeXt-based编码器，设计结合分类和一致性目标的损失函数。

Result: 在Office-31、Office-Home和Office-Caltech数据集上分别达到90.72%、84%和97.12%的平均分类准确率。相比现有方法，分别提高了+1.23%、+7.26%和+1.77%的平均分类准确率。

Conclusion: 该方法成功实现了无需源域数据的域自适应，通过多视图增强和潜在空间一致性技术有效学习域不变特征，在多个数据集上取得了优于现有方法的性能。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [13] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了一个全面的视频生成质量评估协议，专注于外观、运动和相机三个关键感知维度，定义了10种常见伪影类别，并构建了包含8万视频的大规模数据集GenVID，开发了DVAR框架进行细粒度伪影识别。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的快速发展，评估和审计生成视频变得日益重要。现有方法通常只提供粗略的视频质量评分，缺乏对特定伪影的详细定位和分类。

Method: 1) 提出了一个全面的评估协议，专注于影响人类感知的三个关键方面：外观、运动和相机；2) 定义了包含10种常见生成失败的伪影分类法；3) 构建了GenVID数据集，包含8万个由各种先进视频生成模型生成的视频，每个都仔细标注了定义的伪影类别；4) 开发了DVAR（密集视频伪影识别）框架，用于细粒度的生成伪影识别和分类。

Result: 大量实验表明，该方法显著提高了伪影检测的准确性，并能够有效过滤低质量内容。

Conclusion: 该研究为视频生成质量评估提供了一个系统化的框架，通过细粒度的伪影识别和分类，弥补了现有方法在详细定位和分类方面的不足，有助于更准确地评估和改进视频生成模型。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [14] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++：一种基于查询的多模态端到端感知与预测框架，通过查询门控可变形融合技术整合多视角RGB和LiDAR数据，在nuScenes数据集上实现了更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统模块化流水线会限制信息流并放大上游错误，而现有的基于查询的感知预测模型在摄像头和LiDAR的查询空间互补性方面探索不足，融合方案常引入启发式对齐和离散选择步骤，导致信息利用不充分和偏差引入。

Method: 提出查询门控可变形融合（QGDF）技术：(i)通过跨摄像头和特征层的掩码注意力聚合图像证据；(ii)通过具有学习到的每查询偏移量的完全可微分BEV采样提取LiDAR上下文；(iii)应用查询条件门控机制自适应加权每个智能体的视觉和几何线索。该架构在单一端到端模型中联合优化检测、跟踪和多假设轨迹预测。

Result: 在nuScenes数据集上，Li-ViP3D++提高了端到端行为质量和检测质量，实现了更高的EPA（0.335）和mAP（0.502），同时显著降低了误报率（FP ratio 0.147），并且比之前的Li-ViP3D变体更快（139.82 ms vs. 145.91 ms）。

Conclusion: 查询空间中的完全可微分摄像头-LiDAR融合可以在不牺牲部署性的情况下提高端到端感知预测的鲁棒性，表明这种融合方法能够更充分地利用多模态信息。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [15] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: C-SAM框架通过将锐度感知学习从参数扰动转移到掩码扰动，在模型压缩过程中同时优化紧凑性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: SAM能提升DNN对输入变化的鲁棒性，但与传统剪枝方法结合时存在矛盾：先训练后剪枝会破坏鲁棒性，先剪枝后训练则受限于剪枝模式。需要一种能同时优化模型紧凑性和鲁棒性的方法

Method: 提出C-SAM框架，将锐度感知学习从参数扰动转移到掩码扰动。在训练过程中显式扰动剪枝掩码，促进对模型结构更平坦的损失景观，从而发现同时优化紧凑性和鲁棒性的剪枝模式

Result: 在CelebA-HQ、Flowers-102和CIFAR-10-C数据集上，使用ResNet-18、GoogLeNet和MobileNet-V2进行实验，C-SAM相比强基线方法获得高达42%的认证鲁棒性提升，同时保持与未剪枝模型相当的任务准确率

Conclusion: C-SAM通过掩码扰动实现了模型压缩与鲁棒性的协同优化，为设备端DNN部署提供了有效的解决方案

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [16] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 该研究提出了一种双域学习策略，通过结合有/无施源器的CT数据，仅需少量目标域数据即可显著提升膀胱分割在协变量偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中协变量偏移导致的性能下降是主要挑战。在CT引导的妇科近距离放疗膀胱分割中，无施源器(NA)的CT扫描广泛可用，而有施源器(WA)的扫描稀缺且存在解剖变形和伪影，导致自动分割困难。需要解决如何有效利用分布偏移的样本来支持有限目标域数据的学习。

Method: 提出双域学习策略，整合NA和WA CT数据。使用精心策划的混合数据集，系统评估在轴位、冠状位和矢状位平面上，将少量WA数据（10-30%）加入以NA为主的训练集对分割性能的影响。采用多种深度学习架构进行实验验证。

Result: 仅使用NA数据无法捕捉WA图像的特征，但引入少量WA数据（10-30%）即可达到与完全使用WA数据训练模型相当的分割性能。获得Dice相似系数高达0.94，IoU得分高达0.92，表明有效的域适应和临床可靠性提升。

Conclusion: 该研究证明了整合解剖相似但分布偏移的数据集的价值，能够克服数据稀缺问题，提升近距离放疗治疗计划中基于深度学习的分割性能。双域学习策略为协变量偏移下的医学图像分割提供了有效解决方案。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [17] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 从单张RGB图像通过物理结构化框架估计物体质量，结合几何体积和材料密度信息来解决视觉质量估计的歧义问题


<details>
  <summary>Details</summary>
Motivation: 从视觉输入估计物体质量具有挑战性，因为质量取决于几何体积和材料密度，这两者都不能直接从RGB外观中观察到。像素级质量预测是不适定问题，需要物理上有意义的表示来约束解空间。

Method: 提出物理结构化框架：1) 通过单目深度估计恢复物体中心三维几何以获取体积信息；2) 使用视觉语言模型提取粗略材料语义以指导密度相关推理；3) 通过实例自适应门控机制融合几何、语义和外观表示；4) 通过独立回归头预测体积相关和密度相关的物理引导潜在因子，仅使用质量监督。

Result: 在image2mass和ABO-500数据集上的实验表明，该方法始终优于最先进的方法。

Conclusion: 通过物理结构化框架将视觉线索与物理因素对齐，能够有效解决单图像质量估计的歧义问题，在多个数据集上取得优越性能。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [18] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出SLDM模型，通过结构约束和语言引导的扩散模型，从低剂量对比剂CT生成正常剂量CT图像，解决不完全配对图像下的准确增强问题。


<details>
  <summary>Details</summary>
Motivation: 碘对比剂(ICM)在CT检查中能提高敏感性和特异性，但过量使用会导致肾损伤和过敏反应等风险。现有深度学习方法难以在不完全配对的图像上实现准确增强，主要因为模型识别特定结构的能力有限。

Method: 提出结构约束语言引导扩散模型(SLDM)：1)有效提取图像结构先验信息约束模型推理，确保增强过程中的结构一致性；2)引入具有空间智能的语义监督策略，整合视觉感知和空间推理功能；3)应用减影血管增强模块，将ICM区域对比度调整到适合观察的区间。

Result: 通过视觉比较的定性分析和多个指标的定量结果，证明了该方法在低剂量对比剂CT血管造影重建中的有效性。

Conclusion: SLDM模型能够在不完全配对图像的情况下实现准确的对比剂增强，为降低CT检查中对比剂剂量同时保持诊断能力提供了有效解决方案。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [19] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer：首个通过高效一步扩散过程实现真实世界时空视频超分辨率的框架，结合线性预插值、TR-SE MoE专家系统和双向可变形VAE解码器，在保持时间一致性的同时提升空间细节。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频超分辨率中表现出色，但在时空视频超分辨率（STVSR）领域尚未充分探索。现有STVSR方法通常在简化的退化假设下工作，难以应对真实世界中复杂的未知退化问题，需要同时保证重建保真度和时间一致性。

Method: 提出OSDEnhancer框架：1）通过线性预插值策略初始化必要的时空结构；2）训练时间精炼和空间增强混合专家系统（TR-SE MoE），让不同专家路径分别学习时间一致性和空间细节的专门表示；3）引入双向可变形VAE解码器进行循环时空聚合和传播，增强跨帧重建保真度。

Result: 实验表明该方法在真实世界场景中实现了最先进的性能，同时保持了优越的泛化能力。

Conclusion: OSDEnhancer是首个通过高效一步扩散过程实现真实世界时空视频超分辨率的方法，通过创新的架构设计解决了STVSR中的关键挑战，在保持时间一致性的同时提升了空间细节质量。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [20] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi是一个通道排列不变的多变量时间序列预测框架，通过解耦时空架构和排列不变正则化训练，解决了现有通道依赖模型过拟合通道顺序和通道独立模型忽略通道间依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多变量时间序列预测方法存在两难困境：通道依赖模型学习跨通道特征但容易过拟合通道顺序，当通道增加或重排时难以适应；通道独立模型虽然灵活但忽略了通道间依赖关系，限制了性能。需要一种既能学习通道间关系又不依赖于固定通道顺序的方法。

Method: CPiRi采用时空解耦架构和排列不变正则化训练策略：1）冻结的预训练时间编码器提取高质量时间特征；2）轻量级空间模块学习内容驱动的通道间关系；3）通道洗牌策略在训练中强制实现通道排列不变性。理论分析了多变量时间序列预测中的排列等变性。

Result: 在多个基准测试中取得最先进结果。当通道顺序被打乱时保持稳定，即使只在一半通道上训练，对未见通道也表现出强大的归纳泛化能力，同时在大规模数据集上保持实际效率。

Conclusion: CPiRi通过通道排列不变框架解决了通道依赖和通道独立模型的局限性，能够从数据中推断跨通道结构而非记忆固定顺序，在结构和分布共同漂移的环境中无需重新训练即可部署，具有实际应用价值。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [21] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种改进3D高斯泼溅表面重建的方法，通过高斯可见性感知的多视角几何一致性约束和渐进四叉树校准的单目深度约束，解决了现有方法在几何差异大时的不可靠性和尺度模糊问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能实现高效优化和高质量渲染，但准确的表面重建仍然具有挑战性。现有方法通过多视角几何一致性或单目深度先验来改进高斯深度估计，但多视角约束在几何差异大时不可靠，而单目先验存在尺度模糊和局部不一致问题，导致高斯深度监督不准确。

Method: 1. 高斯可见性感知的多视角几何一致性约束：聚合共享高斯基元在不同视角下的可见性，实现更准确稳定的几何监督。
2. 渐进四叉树校准的单目深度约束：从粗到细的空间尺度进行分块仿射校准，缓解深度先验的尺度模糊，同时保留细粒度表面细节。

Result: 在DTU和TNT数据集上的大量实验表明，该方法在几何精度上相比先前的高斯基和隐式表面重建方法都有持续改进。

Conclusion: 提出的方法通过结合可见性感知的多视角几何约束和渐进校准的单目深度约束，有效解决了3D高斯泼溅表面重建中的几何监督不准确问题，实现了更精确的表面重建效果。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [22] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT：一种基于拓扑感知最优传输的异常分割框架，通过多尺度持久图对齐和稳定性评分实现鲁棒的异常检测


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化方法在分布偏移下产生脆弱的掩码，而深度拓扑数据分析能捕捉跨尺度的结构不变量，更适合异常分割。异常应被表征为全局结构的破坏而非局部波动。

Method: 提出TopoOT框架，集成多过滤持久图与测试时适应。核心创新是最优传输链式对齐，跨阈值和过滤序列对齐持久图，产生识别跨尺度一致特征的测地稳定性分数。这些稳定性感知伪标签通过OT一致性和对比目标监督在线训练的轻量级头。

Result: 在标准2D和3D异常检测基准测试中，TopoOT达到最先进性能，在2D数据集上比最具竞争力的方法高出+24.1%平均F1，在3D异常分割基准上高出+10.2%。

Conclusion: TopoOT通过拓扑感知最优传输框架有效整合多尺度拓扑特征，在分布偏移下实现鲁棒的异常分割，证明了拓扑结构分析在异常检测中的优势。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [23] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SpatialGenEval是一个新的文本到图像模型基准测试，专注于评估模型处理复杂空间关系的能力，包含1230个信息密集的长提示和对应的多选问答对，揭示了高阶空间推理是当前模型的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型虽然在生成高保真图像方面取得了显著成功，但在处理复杂空间关系（如空间感知、推理和交互）方面表现不佳。现有基准测试由于提示设计简短或信息稀疏，很大程度上忽视了这些关键方面。

Method: 研究团队开发了SpatialGenEval基准测试，包含1230个信息密集的长提示，覆盖25个真实场景，每个提示整合了10个空间子领域和对应的10个多选问答对。同时构建了SpatialT2I数据集，包含15,400个文本-图像对，通过重写提示确保图像一致性同时保持信息密度。

Result: 对21个最先进模型的广泛评估显示，高阶空间推理仍然是主要瓶颈。在基础模型（Stable Diffusion-XL、Uniworld-V1、OmniGen2）上进行微调后，获得了持续的性能提升（+4.2%、+5.7%、+4.4%），并在空间关系上产生了更真实的效果。

Conclusion: SpatialGenEval基准测试系统地评估了文本到图像模型的空间智能，揭示了当前模型的局限性。通过数据中心的范式，使用信息密集的数据集进行微调可以显著提升模型的空间推理能力，为实现文本到图像模型的空间智能提供了有效途径。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [24] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE是一个因果启发的框架，通过变分不确定性建模和不确定性引导的结构正则化来抑制高方差、环境特定的关系，以提升场景图在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 场景图虽然为场景理解提供了结构化抽象，但往往过度拟合虚假相关性，严重阻碍了分布外泛化能力。需要解决场景图对虚假相关性的过拟合问题，提升其在分布变化下的鲁棒性。

Method: 提出CURVE框架：1）集成变分不确定性建模；2）采用不确定性引导的结构正则化来抑制高方差、环境特定的关系；3）应用原型条件去偏方法，将不变的交互动态与环境依赖的变化解耦，促进稀疏且领域稳定的拓扑结构。

Result: 在零样本迁移和低数据模拟到真实适应任务中进行了实证评估，验证了CURVE能够学习领域稳定的稀疏拓扑，并提供可靠的不确定性估计以支持分布变化下的风险预测。

Conclusion: CURVE通过因果启发的方法有效解决了场景图在分布外泛化中的过拟合问题，通过不确定性建模和结构正则化提升了模型的鲁棒性和可靠性。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [25] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: RAW-Flow：将RGB到RAW重建重新定义为确定性潜在传输问题，使用流匹配学习潜在空间向量场，实现高保真重建


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法将RGB到RAW重建视为直接回归任务，但由于逆ISP的病态性和量化RGB图像的信息损失，存在细节不一致和颜色偏差问题

Method: 提出RAW-Flow框架：1）将RGB到RAW重建重新定义为确定性潜在传输问题；2）使用流匹配学习潜在空间向量场；3）引入跨尺度上下文引导模块注入分层RGB特征；4）设计具有特征对齐约束的双域潜在自编码器

Result: 大量实验表明，RAW-Flow在定量和视觉上都优于现有最先进方法

Conclusion: 通过生成视角重新定义RGB到RAW重建为潜在传输问题，RAW-Flow能够有效解决细节不一致和颜色偏差问题，实现高质量重建

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [26] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT是一个用于多人运动生成的自动回归扩散框架，通过分层交互建模处理可变长度文本和变化人数，在InterHuman基准上FID达到3.100，显著优于之前最佳结果5.154。


<details>
  <summary>Details</summary>
Motivation: 现有离线方法在生成固定长度运动时，难以处理长文本或可变文本以及变化的人数。这些限制促使需要自动回归方法，能够基于过去轨迹和当前文本指导逐步预测未来运动。

Method: HINT采用分层交互建模的扩散框架：1）在规范化潜在空间中使用解耦运动表示，将局部运动语义与人际交互分离；2）采用滑动窗口策略进行高效在线生成，聚合窗口内局部和跨窗口全局条件来捕捉历史、人际依赖和文本对齐。

Result: 在公共基准测试中，HINT与强离线模型性能相当，并超越自动回归基线。在InterHuman上，HINT的FID达到3.100，显著优于之前最佳结果5.154。

Conclusion: HINT是首个用于多人运动生成的自动回归扩散框架，通过分层交互建模有效解决了可变文本长度和变化人数的问题，实现了细粒度交互建模和长序列一致性。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [27] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA通过视图精炼和描述精炼去除视觉-文本对齐中的冗余信息，提升CLIP模型的零样本性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将细粒度文本描述与局部图像块对齐以提升CLIP零样本性能，但发现两者都包含冗余信息，导致对齐效果不佳

Method: 提出BiFTA框架，包含视图精炼（通过高IoU去除冗余图像块）和描述精炼（通过高余弦相似度去除冗余文本描述），保留更具区分性的视觉样本和更多样化的文本描述

Result: 在6个基准数据集上，BiFTA在ViT-based和ResNet-based CLIP上都取得了优越的零样本性能

Conclusion: 去除视觉-文本对齐中的冗余信息对于提升CLIP零样本性能是必要的，BiFTA通过双向精炼有效解决了这一问题

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [28] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: Quartet of Diffusions是一个结构感知的点云生成框架，通过四个协调的扩散模型分别建模全局形状潜在变量、对称性、语义部件及其空间组装，实现对称性保证、部件连贯放置和高质量多样化输出。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将形状生成视为整体过程，要么仅支持部件组合，缺乏对对称性和部件先验的完整集成。需要一种能够显式建模部件组成和对称性，同时支持细粒度控制的生成框架。

Method: 提出四重扩散模型框架：1) 全局形状潜在变量扩散模型；2) 对称性扩散模型；3) 语义部件扩散模型；4) 部件空间组装扩散模型。通过解耦生成过程为可解释组件，使用中心全局潜在变量确保结构连贯性。

Result: 实验表明该方法达到最先进性能，能够保证对称性、实现连贯部件放置，并生成多样化高质量输出。据作者所知，这是首个在生成过程中完全集成并强制执行对称性和部件先验的3D点云生成框架。

Conclusion: Quartet of Diffusions通过结构化管道实现了对称性保证和部件级控制，为3D形状生成提供了可解释且可控的解决方案，在保持全局一致性的同时支持对单个部件的针对性操作。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [29] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing是一个高效的文档解析模型，采用ViT视觉编码器和Youtu-LLM-2B语言模型，通过token并行和query并行解码策略实现5-11倍加速，在多个文档元素解析任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统文档解析方法在处理复杂文档结构时效率较低，需要开发一个既能高效处理多种文档元素（文本、公式、表格、图表等），又能加速解码过程的通用解析模型。

Method: 采用解耦的特征复用框架：1) 动态分辨率ViT视觉编码器提取共享文档特征；2) 提示引导的Youtu-LLM-2B语言模型进行布局分析和区域提示解码；3) 高并行解码策略包括token并行（每步生成64个候选token）和query并行（同时预测多个边界框内容）。

Result: 在OmniDocBench和olmOCR-bench基准测试中达到最先进性能；token并行解码比传统自回归解码快5-11倍，query并行提供额外2倍加速；模型对罕见字符、多语言文本和手写内容具有强鲁棒性。

Conclusion: Youtu-Parsing是一个高效、多功能的文档解析模型，通过创新的并行解码策略显著提升处理速度，在多种文档元素解析任务上表现优异，具有重要的实验价值和实际应用潜力。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [30] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE提出了一种基于视觉语言模型的多模态对齐与强化方法，用于可解释的Deepfake检测，通过RLHF奖励函数和伪造解耦模块提升检测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，Deepfake检测面临新的挑战。现有方法主要将问题建模为分类或空间定位，需要更准确可靠且可解释的检测方法。

Method: MARE设计了包含人类反馈强化学习(RLHF)的综合奖励函数，激励生成文本-空间对齐的推理内容；引入伪造解耦模块从高级面部语义中捕获内在伪造痕迹。

Result: 定量和定性实验结果表明，MARE在准确性和可靠性方面达到了最先进的性能，生成的推理内容经过全面评估。

Conclusion: MARE通过多模态对齐和强化学习，结合伪造解耦技术，显著提升了视觉语言模型在Deepfake检测和推理中的表现。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [31] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出Dummy Forcing方法，通过控制多头自注意力中不同头对历史帧的访问，减少上下文冗余并实现更激进的缓存压缩，在不增加训练的情况下实现2.0倍加速。


<details>
  <summary>Details</summary>
Motivation: 研究发现自回归视频扩散模型中的多头自注意力机制对历史帧利用不足：约25%的注意力头几乎只关注当前帧，丢弃它们的KV缓存只会导致轻微性能下降。

Method: 提出Dummy Forcing方法：1）异构内存分配减少头间上下文冗余；2）动态头编程自适应分类头类型；3）上下文打包技术实现更激进的缓存压缩。

Result: 无需额外训练，Dummy Forcing相比基线实现最高2.0倍加速，支持24.3 FPS的视频生成，质量下降小于0.5%。

Conclusion: Dummy Forcing通过有效控制多头自注意力中不同头对历史帧的访问，显著提升自回归视频扩散模型的推理效率，为实时视频生成提供了实用解决方案。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [32] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 该研究探讨了在部分标注数据上训练联合白质高信号和缺血性卒中病灶分割模型的六种策略，发现使用伪标签的方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 白质高信号和缺血性卒中病灶是脑小血管病的影像学特征，在FLAIR序列中容易相互混淆且常同时出现。开发能够同时分割和区分这两种特征的深度学习模型具有挑战性，因为通常缺乏完全标注的数据。

Method: 研究者调查了六种使用部分标注数据训练联合分割模型的策略。结合了私有完全标注数据集、私有部分标注数据集和公开部分标注数据集，共2052个MRI体积。比较了不同方法在利用部分标注数据方面的效果。

Result: 研究发现多种方法能够有效利用部分标注数据提升模型性能，其中使用伪标签的方法取得了最佳结果。

Conclusion: 通过合理利用部分标注数据，可以开发出有效的联合分割模型，伪标签策略在利用部分标注数据方面表现最为出色。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [33] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在时间差异（LTD）的运动感知损失加权方法，用于改善视频生成模型在动态场景中的性能，特别是在剧烈运动变化时保持时间一致性和动态区域质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在静态场景表现良好，但在动态视频生成中质量下降，特别是在剧烈动态变化时。这是因为噪声破坏了时间一致性，增加了动态区域学习的难度。现有的扩散模型对所有场景都使用静态损失，限制了其捕捉复杂动态的能力。

Method: 提出潜在时间差异（LTD）作为运动先验来指导损失加权。LTD在潜在空间中测量帧间变化，对差异较大的区域分配更大的惩罚权重，同时对稳定区域保持常规优化。这种运动感知策略稳定了训练，使模型能更好地重建高频动态。

Result: 在通用基准VBench和专注于运动的VMBench上进行广泛实验，结果显示该方法在VBench上比强基线提升3.31%，在VMBench上提升3.58%，在运动质量方面取得了显著改进。

Conclusion: 提出的LTD运动感知损失加权方法有效解决了视频生成模型在动态场景中的性能限制，通过运动先验指导损失分配，显著提升了模型在复杂动态变化下的生成质量。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [34] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 本文提出了肖像集合生成（PCG）新任务，通过自然语言指令编辑参考肖像图像来生成连贯的肖像集合，并构建了首个大规模数据集CHEESE和SCheese框架来解决多属性修改和细节保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体平台激增，用户需要直观的方式来创建多样化、高质量的肖像集合。现有方法面临两个主要挑战：复杂的多属性修改（如姿势、空间布局、相机视角）以及高保真细节保持（如身份、服装、配饰）。

Method: 提出SCheese框架，结合文本引导生成与分层身份和细节保持。采用自适应特征融合机制保持身份一致性，使用ConsistencyNet注入细粒度特征保持细节一致性。同时构建了CHEESE数据集，包含24K肖像集合和573K样本，通过大型视觉语言模型管道和基于反转的验证构建。

Result: 综合实验验证了CHEESE数据集在推进PCG任务方面的有效性，SCheese框架实现了最先进的性能表现。

Conclusion: 本文提出了肖像集合生成新任务，通过构建大规模数据集和提出有效框架，成功解决了多属性修改和细节保持的挑战，为社交媒体肖像创作提供了新的解决方案。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [35] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 本文提出CoTA方法解决扩散式多模态大语言模型中的重复文本生成问题（Repeat Curse），通过分析信息流发现重复生成与上下文令牌信息流中断和熵不收敛相关，CoTA通过增强上下文令牌注意力并引入惩罚项来缓解重复。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大语言模型（dMLLMs）依赖缓存技术加速解码，但这会引入重复文本生成问题（Repeat Curse）。作者旨在深入探究这一问题的底层机制，并提出有效的解决方案。

Method: 从信息流角度分析重复生成机制，发现三个关键发现：1）上下文令牌作为锚点聚合语义信息指导最终预测；2）信息跨层传播时，上下文令牌熵在深层收敛；3）重复与上下文令牌信息流中断和熵不收敛相关。基于此提出CoTA方法：增强上下文令牌注意力以保持信息流模式，并在解码时引入惩罚项避免不确定上下文令牌驱动的输出。

Result: CoTA在缓解重复生成方面表现出显著效果，并在通用任务上实现了一致的性能提升。代码已开源。

Conclusion: 通过信息流分析揭示了重复生成的机制，提出的CoTA方法能有效缓解扩散式多模态大语言模型中的重复问题，且具有即插即用特性，在保持模型性能的同时提升生成质量。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [36] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA是一个新颖的黑白盒提示学习框架，通过结合数据驱动的黑盒模块和知识驱动的白盒模块，实现下游任务的有效适应。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效调优方法将预训练模型视为不透明的黑盒，仅依赖数据驱动优化，未能充分利用其内在先验知识，限制了模型在下游任务适应中的潜力。

Method: 提出IOTA框架，包含数据驱动的黑盒模块和知识驱动的白盒模块。白盒模块通过对比错误预测与正确认知来推导纠正知识，将其转化为可解释的人类提示，并通过纠正知识引导的提示选择策略来指导黑盒模块做出更准确的预测。

Result: 在12个图像分类基准测试中，在少样本和易到难适应设置下，实验结果表明纠正知识的有效性以及该方法相对于最先进方法的优越性。

Conclusion: 通过联合利用知识和数据驱动的学习信号，IOTA实现了有效的下游任务适应，克服了传统参数高效调优方法的局限性。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [37] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World是一个基于视频生成的开源世界模拟器，具有高保真度、长时记忆和实时交互能力，支持多种环境风格。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小开源与闭源技术之间的差距，为内容创作、游戏和机器人学习等领域提供实用的世界模拟工具。

Method: 基于视频生成技术构建世界模拟器，通过优化算法实现高保真度、长时记忆和低延迟交互。

Result: 开发出具有高保真度、支持分钟级时间跨度、保持上下文一致性、实现1秒内延迟（16帧/秒）的实时交互世界模拟器。

Conclusion: LingBot-World作为顶级世界模型，通过开源代码和模型，将推动社区在多个应用领域的发展。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [38] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2提出了一种新颖的视觉编码器DeepEncoder V2，能够根据图像语义动态重排视觉标记，模仿人类视觉的因果推理扫描模式，为复杂布局图像理解提供新架构。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型采用固定的光栅扫描顺序处理视觉标记，这与人类视觉感知的灵活语义驱动扫描模式相矛盾。人类视觉对复杂布局图像表现出基于因果推理的顺序处理能力，现有模型缺乏这种认知机制。

Method: 提出DeepEncoder V2编码器，赋予编码器因果推理能力，使其能够在基于LLM的内容解释之前智能地重排视觉标记。探索通过两个级联的1D因果推理结构实现2D图像理解的新范式。

Result: 开发了DeepSeek-OCR 2模型，代码和模型权重已公开在GitHub上，为图像理解提供了新的架构方法，有望实现真正的2D推理。

Conclusion: 通过模仿人类视觉的因果推理扫描模式，动态重排视觉标记的方法为复杂图像理解提供了有前景的新方向，探索了通过级联1D因果结构实现2D理解的新范式。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [39] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: DiffVC-RT：首个实现实时扩散神经视频压缩的框架，通过高效架构设计、显隐式一致性建模和异步并行解码，在保持高质量的同时大幅提升速度


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的神经视频压缩面临信息丢失严重、推理延迟高、时间一致性差等关键挑战，限制了实际部署应用

Method: 1. 高效信息模型架构：通过模块替换和剪枝降低计算复杂度；2. 显隐式一致性建模：零成本在线时间偏移模块增强时间一致性；3. 异步并行解码流水线：混合半精度和批维度时间偏移设计实现并行处理

Result: 在HEVC数据集上，相比VTM-17.0节省80.1%的LPIPS比特率，在NVIDIA H800 GPU上实现720p视频206fps编码和30fps解码的实时性能

Conclusion: DiffVC-RT是扩散基视频压缩领域的重大里程碑，首次实现了实时性能，为实际部署铺平了道路

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [40] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出StructAlign方法，通过引入等角紧框架几何先验和跨模态关系保持损失，解决连续文本-视频检索中的特征漂移问题，有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 连续文本-视频检索面临灾难性遗忘的严重挑战，主要问题在于特征漂移：包括模态内特征漂移和跨模态非合作特征漂移导致的模态不对齐。

Method: 提出StructAlign方法：1) 引入单纯形等角紧框架作为统一几何先验，设计跨模态ETF对齐损失，将文本和视频特征与类别级ETF原型对齐；2) 设计跨模态关系保持损失，利用互补模态保持跨模态相似性关系，抑制模态内特征漂移。

Result: 在基准数据集上的大量实验表明，该方法在连续文本-视频检索任务中持续优于最先进的连续检索方法。

Conclusion: 通过同时解决跨模态非合作特征漂移和模态内特征漂移，StructAlign有效缓解了连续文本-视频检索中的灾难性遗忘问题。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [41] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 该论文评估了三种训练范式（监督学习、自监督学习、语言对齐模型）在行人重识别任务中的表现，特别关注跨域泛化能力，发现语言对齐模型在跨域场景中表现出意外的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 行人重识别在计算机视觉中仍具挑战性，研究旨在评估不同训练范式在跨域应用中的鲁棒性，探索基础模型通过更丰富、可迁移的视觉表征提升泛化能力的作用。

Method: 比较三种训练范式：监督学习、自监督学习和语言对齐模型。在11个模型和9个数据集上进行实验分析，评估它们在跨域场景中的表现。

Result: 监督模型在训练域内表现优异但在跨域数据上崩溃；语言对齐模型尽管未针对重识别任务进行显式训练，却在跨域场景中展现出令人惊讶的鲁棒性。

Conclusion: 语言对齐模型在行人重识别任务中具有更好的跨域泛化能力，为未来研究提供了新的方向，表明基础模型能够提供更丰富、可迁移的视觉表征。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [42] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon：首个针对长时程重复性工作流程的评估基准，包含242个任务，并提出基于少量示例构建浓缩演示的方法来有效指导智能体执行大规模数据处理


<details>
  <summary>Details</summary>
Motivation: 专业环境中存在大量长时程、重复性工作流程（如处理报销单、录入学生成绩），这些任务对人类来说单调乏味，但对计算机使用智能体（CUAs）却很理想。当前缺乏评估此类任务的基准是主要瓶颈。

Method: 1）建立OS-Marathon基准，包含242个长时程重复性任务，涵盖2个领域；2）提出成本效益高的方法，仅使用少量示例构建浓缩演示，教会智能体底层工作流程逻辑，使其能在未见的大规模数据集合上有效执行类似工作流程。

Result: 大量实验证明了这些任务的内在挑战性以及所提方法的有效性。OS-Marathon基准揭示了当前最先进智能体在长时程重复性任务上的表现局限，而浓缩演示方法能显著提升智能体执行大规模重复性工作流程的能力。

Conclusion: OS-Marathon填补了长时程重复性工作流程评估基准的空白，提出的浓缩演示方法为智能体学习复杂工作流程逻辑提供了高效解决方案，有助于推动计算机使用智能体在实际专业场景中的应用。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [43] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: ProSkill是首个用于程序性任务中动作级别技能评估的基准数据集，提供绝对和成对技能评估标注，通过创新的标注协议和瑞士锦标赛方案创建连续全局评分。


<details>
  <summary>Details</summary>
Motivation: 当前技能评估研究主要集中于体育领域，缺乏复杂程序性活动的大规模数据集，现有研究通常只涉及有限动作，且仅关注成对评估或二元标签，无法满足程序性视频中技能评估的需求。

Method: 提出创新的可扩展标注协议，采用瑞士锦标赛方案进行高效成对比较，然后使用ELO评分系统将成对评估聚合成一致、连续的全局技能分数，创建ProSkill数据集。

Result: 创建了ProSkill基准数据集，用于评估现有最先进的技能评估算法（包括基于排名和成对范式的方法），当前最先进算法在该数据集上表现不佳，凸显了该数据集的挑战性和价值。

Conclusion: ProSkill填补了程序性视频技能评估领域的数据集空白，其创新的标注协议和评估框架为复杂程序性活动的技能评估提供了有价值的基准，推动了该领域的研究发展。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [44] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF是一个标签高效的图像质量评估框架，通过从MLLM教师模型中提取感知质量先验知识到轻量级学生回归器，显著减少了对人工标注的需求


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在图像质量评估任务中表现出色，但适应这些大规模模型计算成本高，且仍依赖大量人工标注的平均意见分数。作者认为MLLM-based IQA的核心瓶颈不在于MLLMs的质量感知能力，而在于MOS尺度校准

Method: 提出LEAF框架，从MLLM教师模型中提取感知质量先验知识到轻量级学生回归器。教师模型通过点级判断和配对偏好进行密集监督，并估计决策可靠性。学生模型通过联合蒸馏学习教师的质量感知模式，并在小型MOS子集上进行校准以对齐人类标注

Result: 在用户生成和AI生成的IQA基准测试中，该方法显著减少了对人工标注的需求，同时保持了强大的MOS对齐相关性，使轻量级IQA在有限标注预算下变得实用

Conclusion: LEAF框架通过有效的知识蒸馏和最小化的人工监督，解决了MLLM-based IQA中的MOS尺度校准瓶颈问题，为有限标注预算下的轻量级图像质量评估提供了实用解决方案

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [45] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON是一个基于STEM讲座视频的多模态理解评估基准，专注于长时程推理和跨模态整合，包含2,277个视频片段和4,181个高质量QA对，揭示了当前MLLMs在时间推理和教学预测方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉、音频和语言任务上取得了显著进展，但在长形式、知识密集且具有时间结构的教学内容上的表现尚未得到充分探索。需要建立一个专门的评估基准来填补这一空白。

Method: 提出了LEMON基准，包含2,277个STEM讲座视频片段，涵盖5个学科和29门课程，平均时长196.1秒。生成了4,181个高质量QA对，包括3,413个多项选择题和768个开放性问题。基准具有六个主要任务和十二个子任务，覆盖从感知到推理再到生成的完整认知谱系。

Result: 综合实验显示各任务间存在显著性能差距，即使是GPT-4o等最先进的MLLMs在时间推理和教学预测方面也表现不佳。基准揭示了当前模型在长形式教学内容理解方面的局限性。

Conclusion: LEMON作为一个可扩展且具有挑战性的基准，有望推动多模态感知、推理和生成在长形式教学内容中的发展，为评估和改进MLLMs在教育领域的应用提供重要工具。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [46] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 该论文探讨了视觉编码与视觉token技术在压缩效率与模型性能权衡上的本质统一性，并展望了下一代视觉编解码器和token技术的发展方向。


<details>
  <summary>Details</summary>
Motivation: 在人工智能领域，压缩效率通常与模型性能提升相关。传统视觉编码技术已发展数十年并广泛应用，而新兴的生成式多模态大模型的视觉token技术也追求类似目标：在表示学习中最大化语义信息保真度同时最小化计算成本。作者希望统一这两种技术家族，探讨其背后的优化本质。

Method: 首先全面概述视觉编码和视觉token技术两大主导技术家族，然后从优化角度统一它们，讨论压缩效率与模型性能权衡的本质。基于提出的统一公式，综合双向见解并预测下一代视觉编解码器和token技术。最后通过实验展示面向任务的token开发在多模态LLMs、AIGC和具身AI等实际任务中的巨大潜力。

Result: 论文提出了统一视觉编码和视觉token技术的公式，揭示了它们在压缩效率与模型性能权衡上的本质相似性。实验表明面向任务的token技术在MLLMs、AIGC和具身AI等实际任务中具有巨大潜力，并展望了未来可能像传统编解码器（如H.264/265）一样标准化通用token技术。

Conclusion: 视觉编码和视觉token技术在优化本质上具有统一性，都追求在压缩表示中最大化语义信息保真度。这种统一视角为下一代视觉编解码器和token技术的发展提供了新思路，有望实现像传统编解码器一样高效、统一的通用token技术，服务于广泛的智能任务。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [47] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V是一个无需训练的去偏框架，通过锚点球面测地变换中和文本编码器中的性别偏见，在保持语义和时序一致性的同时减少文本到视频生成中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 文本到视频（T2V）扩散模型虽然进展迅速，但其人口统计学偏见（特别是性别偏见）尚未得到充分探索。研究发现这些偏见主要来自预训练的文本编码器，即使对于中性提示词也会编码隐含的性别关联。

Method: FairT2V采用无需微调的方法：1）通过锚点球面测地变换中和提示词嵌入，消除偏见同时保持语义；2）通过动态去噪调度，仅在早期身份形成阶段应用去偏，保持时序一致性；3）提出结合VideoLLM推理和人工验证的视频级公平性评估协议。

Result: 在Open-Sora模型上的实验表明，FairT2V显著减少了跨职业的人口统计学偏见，同时对视频质量影响最小。通过性别倾向评分量化了偏见减少的效果。

Conclusion: FairT2V是一个有效的训练免费去偏框架，能够显著减少文本到视频生成中的性别偏见，同时保持视频质量和时序一致性，为公平AI生成提供了实用解决方案。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [48] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI是一个无需训练的功能驱动框架，能从开放词汇任务提示生成功能正确的3D人-场景交互，通过功能感知接触推理和阶段优化确保物理合理性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对物体功能和相应人-场景接触的显式推理，导致生成不可信或功能错误的交互。需要解决3D人类与3D场景功能交互的问题，应用于具身AI、机器人和交互内容创建。

Method: FunHSI采用训练免费框架：1) 功能感知接触推理识别功能场景元素并重建3D几何；2) 通过接触图建模高层交互；3) 利用视觉语言模型合成执行任务的人类图像并估计3D身体和手部姿态；4) 通过阶段优化细化3D身体配置确保物理合理性和功能正确性。

Result: FunHSI不仅合成更合理的通用3D交互（如"坐在沙发上"），还支持细粒度功能交互（如"提高室温"）。在多样室内外场景中一致生成功能正确且物理合理的人-场景交互。

Conclusion: FunHSI通过功能驱动方法解决了3D人-场景交互的关键挑战，实现了从开放词汇任务提示生成功能正确、物理合理的交互，超越了现有方法的能力范围。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [49] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出了一种用于道路表面分类的多模态框架，融合图像和惯性测量数据，使用轻量级双向交叉注意力模块和自适应门控层，在领域偏移下调整模态贡献，并在新数据集ROAD上验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有道路表面分类技术由于传感模态有限和数据集缺乏环境多样性，往往无法在狭窄操作条件之外泛化。需要解决这些限制以支持环境感知的预测性维护系统。

Method: 引入多模态框架，融合图像和惯性测量数据，采用轻量级双向交叉注意力模块和自适应门控层来调整模态贡献。创建ROAD数据集，包含三个互补子集：真实世界多模态记录、大规模视觉子集和合成子集。

Result: 在PVS基准测试上比先前最先进方法提升1.4个百分点，在多模态ROAD子集上提升11.6个百分点，在少数类别上F1分数持续更高。在夜间、大雨和混合表面过渡等挑战性视觉条件下表现稳定。

Conclusion: 结合经济型摄像头和IMU传感器与多模态注意力机制，为道路表面理解提供了可扩展、稳健的基础，特别适用于环境多变性和成本限制限制高端传感套件采用的地区。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [50] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix是一种无需微调的3D场景渲染方法，利用预训练图像扩散模型提升外推视图质量，通过2D-3D交替优化和像素级置信度掩码实现高保真度和泛化能力的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经辐射场和3D高斯泼溅的方法需要密集输入且在外推视图上表现不佳。基于扩散模型的方法面临泛化能力与保真度的权衡：微调方法可能过拟合，而不微调的方法保真度较低。

Method: 提出FreeFix方法：1) 使用预训练图像扩散模型进行2D-3D交替优化，避免使用昂贵的视频扩散模型；2) 引入像素级置信度掩码识别不确定区域进行针对性改进。

Result: 在多个数据集上的实验表明，FreeFix提升了多帧一致性，性能达到或超过基于微调的方法，同时保持了强大的泛化能力。

Conclusion: FreeFix通过创新的2D-3D交替优化策略和置信度掩码，在不进行微调的情况下有效平衡了3D场景渲染的保真度和泛化能力，为外推视图合成提供了新思路。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [51] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为E2HIL的样本高效人机协同强化学习框架，通过主动选择信息丰富的样本来减少人类干预，在真实世界操作任务中实现了更高的成功率和更低的人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协同强化学习框架样本效率低，需要大量人类干预才能收敛，导致高昂的人工成本。需要开发更高效的框架来减少人类干预需求。

Method: 提出E2HIL框架，通过稳定降低策略熵来改善探索与利用的平衡。首先构建不同样本对策略熵的影响函数，通过动作概率和策略软优势的协方差进行高效估计。然后选择影响函数值适中的样本，剪除导致熵急剧下降的捷径样本和影响可忽略的噪声样本。

Result: 在四个真实世界操作任务上的实验表明，E2HIL相比最先进的人机协同强化学习方法实现了42.1%更高的成功率，同时需要10.1%更少的人类干预。

Conclusion: E2HIL框架通过主动选择信息丰富的样本，显著提高了人机协同强化学习的样本效率，在减少人类干预的同时实现了更好的性能，为复杂真实世界操作任务的在线学习提供了有效解决方案。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [52] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过即时模块和运动性能模块改进高维机器人路径规划，在复杂多障碍环境中比传统采样方法更高效


<details>
  <summary>Details</summary>
Motivation: 传统采样方法在高维机器人路径规划中难以在复杂多障碍环境中同时找到可行且最优的路径，特别是在机器人操作臂中，运动奇异性风险和自我碰撞问题进一步影响运动效率和安全

Method: 提出JIT*算法，包含两个核心模块：即时模块（包含动态优化边连接的"即时边"和调整瓶颈区域采样密度的"即时采样"）和运动性能模块（通过动态切换平衡可操作性和轨迹成本）

Result: JIT*在R^4到R^16维度上始终优于传统采样规划器，在单臂和双臂操作任务中表现出有效性

Conclusion: JIT*算法通过创新的即时采样和运动性能优化机制，有效解决了高维机器人路径规划中的效率和安全性问题

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [53] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 提出一种基于二阶校正的高斯过程模型更新方法，用于处理移动机器人定位不确定性导致的测量位置误差问题


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多标量场映射应用依赖移动机器人收集测量数据，但机器人定位不完美会引入状态不确定性，导致测量位置估计不准确，从而降低高斯过程模型的均值和协方差估计质量

Method: 利用核函数的可微性，开发了基于预计算雅可比矩阵和海森矩阵的二阶校正算法，当获得改进的位置估计时，能够实时细化高斯过程模型，而无需完全重新训练

Result: 仿真结果表明，与完全重新训练模型相比，该方法提高了预测精度和计算效率

Conclusion: 该方法有效解决了移动机器人定位不确定性对高斯过程模型的影响，通过二阶校正实现了实时模型更新，在保持计算效率的同时提高了预测准确性

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [54] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER是一个纹理鲁棒的仿射链式思维框架，用于可变形物体的功能区域识别，通过层次语义推理和物理一致性优化来解决现有方法在边界溢出和区域碎片化方面的问题。


<details>
  <summary>Details</summary>
Motivation: 可变形物体操作中的核心挑战在于将高层语义指令与物理交互点对齐，由于自由度无限、动力学复杂和模式异质，现有视觉仿射预测方法存在边界溢出和功能区域碎片化问题。

Method: 提出TRACER框架：1) 树状仿射链式思维(TA-CoT)将任务意图分解为层次子任务语义；2) 空间约束边界细化(SCBR)机制抑制预测溢出；3) 交互收敛细化流(ICRF)聚合被外观噪声污染的离散像素。

Result: 在Fine-AGDDO15数据集和真实机器人平台上的实验表明，TRACER显著提高了跨不同纹理和模式的仿射定位精度，并增强了长时程任务的成功率。

Conclusion: TRACER有效弥合了高层语义推理与低层物理执行之间的鸿沟，为可变形物体操作提供了纹理鲁棒且物理一致的解决方案。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [55] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide：一种新颖的跨策略视觉-触觉融合范式，通过两阶段推理引导预训练的扩散或流匹配视觉运动策略，利用触觉反馈提升精细接触式操作性能


<details>
  <summary>Details</summary>
Motivation: 机器人精细化和接触丰富的操作仍然具有挑战性，主要原因是触觉反馈利用不足。现有方法未能充分利用触觉信息来指导动作生成，特别是在需要精确物理接触的任务中。

Method: TouchGuide采用两阶段推理范式：第一阶段使用视觉输入生成粗略的视觉合理动作；第二阶段通过任务特定的接触物理模型（CPM）提供触觉指导，使用对比学习在有限专家演示上训练，为采样过程提供触觉感知的可行性评分。同时开发了TacUMI数据收集系统，通过刚性指尖获取直接触觉反馈，实现高质量低成本数据收集。

Result: 在五个具有挑战性的接触丰富任务（如系鞋带和芯片交接）上的广泛实验表明，TouchGuide在性能上持续且显著优于最先进的视觉-触觉策略。

Conclusion: TouchGuide通过将触觉指导融入动作空间，有效解决了机器人精细接触式操作的挑战，证明了触觉反馈在提升机器人操作能力中的关键作用，同时TacUMI系统为高质量触觉数据收集提供了经济高效的解决方案。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [56] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi是一个知识蒸馏框架，通过将VLA模型的transformer层数从18层压缩到6层，实现2倍以上的推理加速，在标准操作基准上成功率下降不到1%。


<details>
  <summary>Details</summary>
Motivation: 实时机器人部署需要快速、设备端推理的视觉-语言-动作模型。现有研究主要集中在token级效率优化（如视觉token剪枝），而系统性的transformer层减少在基于流的VLA模型中尚未在知识蒸馏框架下得到充分探索。

Method: 提出Shallow-pi知识蒸馏框架，激进地减少VLM主干和基于流的动作头的transformer深度，将模型从18层压缩到6层。

Result: 实现超过2倍的推理加速，在标准操作基准上成功率绝对下降不到1%，在压缩的VLA模型中达到最先进的性能。在Jetson Orin和Jetson Thor平台上通过工业级真实世界实验验证，包括人形机器人系统在复杂动态操作场景中的表现。

Conclusion: Shallow-pi通过系统性的transformer层减少和知识蒸馏，为实时机器人部署提供了高效、设备端可部署的VLA模型解决方案，在保持性能的同时显著提升推理速度。

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [57] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: 提出TaF-VLA框架，通过触觉-力对齐而非触觉-视觉对齐，将高维触觉观测与物理交互力关联，提升VLA模型在接触密集型任务中的物理直觉和力调节能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型主要依赖视觉模态，缺乏接触密集型任务所需的物理直觉和精确力调节能力。现有将触觉传感融入VLA模型的方法通常将触觉输入视为辅助视觉纹理，忽视了表面变形与交互动力学之间的内在关联。

Method: 提出触觉-力对齐范式，开发自动触觉-力数据采集设备，构建包含超过1000万同步触觉观测、6轴力/扭矩和矩阵力图的TaF数据集。核心是触觉-力适配器，提取离散化潜在信息编码触觉观测，确保学习表示捕捉历史依赖、噪声不敏感的物理动力学而非静态视觉纹理。

Result: TaF-VLA策略在真实世界实验中显著优于最先进的触觉-视觉对齐和纯视觉基线，在接触密集型任务中表现优异，验证了其通过跨模态物理推理实现鲁棒、力感知操作的能力。

Conclusion: 从触觉-视觉对齐转向触觉-力对齐的范式转变能有效增强VLA模型的物理直觉，触觉-力适配器机制使模型能够学习历史依赖的物理动力学，为接触密集型机器人操作任务提供了更有效的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [58] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将前沿LLM智能体框架直接应用于机器人操作，无需任务演示或微调，在多个基准测试中接近VLA模型性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型需要任务特定演示和微调，且领域迁移能力差。研究通用LLM智能体框架是否能作为机器人操作的控制范式替代方案

Method: FAEA将未修改的Claude Agent SDK应用于机器人操作，利用与软件调试相同的迭代推理机制，通过特权环境状态访问进行策略推理

Result: 在LIBERO、ManiSkill3和MetaWorld基准测试中分别达到84.9%、85.7%和96%成功率，接近VLA模型性能；加入一轮人类反馈后LIBERO提升至88.2%

Conclusion: 通用智能体框架足以处理需要深思熟虑任务级规划的操作任务，为机器人系统利用前沿模型基础设施开辟了新路径

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [59] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID：首个开源、大规模、宽频带、几何多样化的射频数据集，用于细粒度材料识别，包含16个细粒度类别、142k样本，支持频率域和时域表示，并建立了多设置、多协议的深度学习基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别受限于光学传感器固有约束，而基于射频的方法虽能揭示材料内在属性但缺乏大规模公开数据集和学习方法的基准测试，阻碍了该领域发展。

Method: 创建RF-MatID数据集，包含16个细粒度材料类别（分为5个超类），频率范围4-43.5GHz，142k样本，系统控制几何扰动（入射角和距离变化），建立多设置、多协议基准，评估最先进深度学习模型。

Result: 提供了首个开源、大规模、宽频带、几何多样化的射频材料识别数据集，建立了包含5种频率分配协议的系统基准，支持分布内性能和分布外鲁棒性评估（跨角度和跨距离偏移）。

Conclusion: RF-MatID旨在促进可重复研究、加速算法进展、增强跨域鲁棒性，并支持射频材料识别在实际应用中的发展，填补了该领域数据集和基准测试的空白。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [60] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM提出了一种轻量级的基于槽的对象中心表示方法，通过多阶段训练策略增强冻结的视觉基础模型，用于机器人操作任务


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型为机器人提供了强大的感知特征，但其密集表示缺乏明确的对象级结构，限制了在操作任务中的鲁棒性和可收缩性

Method: STORM采用多阶段训练策略：首先通过语言嵌入进行视觉-语义预训练来稳定对象中心槽，然后与下游操作策略联合适应，避免槽退化并保持语义一致性

Result: 在对象发现基准和模拟操作任务中，STORM相比直接使用冻结基础模型特征或端到端训练对象中心表示，在视觉干扰泛化和控制性能方面表现更好

Conclusion: 多阶段适应是将通用基础模型特征转化为任务感知对象中心表示的高效机制，适用于机器人控制

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [61] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一个用于多机器人月球勘探任务评估的结构化KPI框架，旨在解决现有现场试验因平台和实验设置差异而难以比较的问题。


<details>
  <summary>Details</summary>
Motivation: 月球关键资源勘探需要鲁棒的探索方法，但现有的模拟现场试验由于机器人平台和实验设置的差异，结果难以比较。这些任务通常使用特定场景的工程指标评估性能，未能建立现场性能与科学目标之间的明确联系。

Method: 从三个现实的多机器人月球场景中推导出结构化KPI框架，这些场景反映了科学目标和操作约束。该框架强调效率、鲁棒性和精度的场景依赖性优先级，并专门为现场部署的实际应用而设计。

Result: 在多机器人现场测试中验证了该框架，发现效率和鲁棒性相关的KPI实用且易于应用，而精度导向的KPI需要可靠的地面真实数据，这在户外模拟环境中并不总是可行获取。

Conclusion: 该框架可作为通用评估标准，实现多机器人现场试验的一致、目标导向比较，并支持未来行星探索机器人系统的系统化开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [62] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 通过低成本压电麦克风和音频谱图Transformer，实现机器人全身触觉定位，静态误差小于5mm，材料特性影响显著，系统对机器人自身运动具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤昂贵且集成复杂，需要一种可扩展、低成本的替代方案来实现机器人全身接触感知。

Method: 在机器人手上安装7个低成本压电麦克风，利用音频谱图Transformer解码物理交互产生的振动信号，分析不同材料特性对定位性能的影响。

Result: 静态条件下定位误差小于5mm；硬质材料（如金属）在脉冲响应定位中表现优异，纹理材料（如木材）在轨迹跟踪中提供更好的摩擦特征；系统对机器人自身运动具有鲁棒性。

Conclusion: 复杂物理接触动力学可以从简单振动信号有效解码，为实现广泛、经济实惠的机器人接触感知提供了可行途径，所有数据集、模型和实验设置已开源。

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [63] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo是一个基于相似性感知的多机器人协作框架，采用"缓存与重用"原则减少冗余计算，通过相似性测试方法重用先前任务解决方案，显著降低规划成本并提高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多机器人协作方法在遇到相同或相似任务时需要从头重新规划，忽略了任务级相似性，导致效率低下。需要解决相似但非相同任务的重用挑战。

Method: 提出MeCo框架，引入相似性测试方法检索高相关性的先前已解决任务，实现无需重新调用LLMs的有效规划重用。同时创建MeCoBench基准测试评估相似任务协作性能。

Result: 实验结果表明，MeCo相比最先进方法显著降低了规划成本并提高了成功率。

Conclusion: MeCo通过相似性感知和缓存重用机制有效解决了多机器人协作中的冗余计算问题，为LLM驱动的协作系统提供了更高效的解决方案。

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [64] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO是一种用于足式机器人强化学习训练的新框架，通过时变动作变换限制早期动作空间以促进有效数据收集，然后逐步扩展动作空间来增强探索，在扭矩控制中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足式机器人的强化学习训练面临高维连续动作、硬件限制和有限探索的挑战。现有方法在位置控制中表现良好，但在扭矩控制中效果较差，因为扭矩控制需要更有效的动作空间探索和梯度信号获取。

Method: 提出Growing Policy Optimization (GPO)框架，应用时变动作变换：早期限制有效动作空间以促进有效数据收集和策略学习，然后逐步扩展动作空间以增强探索并提高预期回报。证明该变换保持了PPO更新规则，仅引入有界、可消失的梯度失真。

Result: 在四足和六足机器人上评估GPO，包括将仿真训练策略零样本部署到硬件上。使用GPO训练的策略始终获得更好的性能表现。

Conclusion: GPO为学习足式运动提供了一个通用、环境无关的优化框架，在扭矩控制中表现出色，能够有效解决动作空间探索的挑战。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [65] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 提出一种从肌腱位移和张力估计关节位置的计算方法，用于无关节编码器的仿人机器人手，通过非线性优化和雅可比PI控制器实现闭环控制


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动的仿人机器人手通常缺乏直接关节角度传感，因为集成关节编码器会影响机械紧凑性和灵巧性。需要一种从肌腱状态估计关节位置的方法。

Method: 1. 基于Denavit-Hartenberg约定建立仿人手的运动学建模框架；2. 使用简化肌腱模型推导肌腱状态与关节位置的非线性方程组；3. 通过非线性优化方法求解；4. 采用雅可比比例积分控制器加前馈项进行闭环控制。

Result: 在MuJoCo仿真环境中使用解剖学正确的生物机电手（每个长指5自由度，拇指6自由度）验证了所提估计和控制框架的有效性和局限性。

Conclusion: 提出了一种无需直接关节传感即可实现手势跟踪的关节位置估计和控制框架，为肌腱驱动仿人机器人手提供了一种替代关节编码器的解决方案。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [66] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO是一个统一的框架，通过MeanFlow实现真正的一步生成，结合分散正则化和RL微调，在机器人控制任务中达到实时性能（>120Hz），推理速度提升5-20倍。


<details>
  <summary>Details</summary>
Motivation: 实时机器人控制需要快速的动作生成，但现有的基于扩散和流匹配的生成策略需要多步采样，这在时间关键场景中部署受到根本限制。

Method: 提出DMPO框架，包含三个关键组件：1) MeanFlow - 通过数学推导实现无需知识蒸馏的单步推理；2) 分散正则化 - 防止表示崩溃；3) RL微调 - 超越专家演示。采用轻量级模型架构。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中，相比多步基线方法表现出竞争性或更优性能。推理速度提升5-20倍，达到>120Hz的实时控制要求，在高性能GPU上可达数百赫兹。在Franka-Emika-Panda机器人上的物理部署验证了实际应用性。

Conclusion: DMPO通过统一框架实现了真正的一步生成，解决了实时机器人控制中的速度瓶颈，在保持性能的同时显著提升了推理效率，具有实际部署价值。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [67] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 该论文提出了一种在ROS 2系统中实现知识图谱的综合方法，旨在提高自主机器人任务的效率和智能性，并通过Aerostack2框架中的模拟搜救任务进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人系统在复杂任务执行中面临决策效率低、任务规划不够智能的问题。论文旨在通过知识图谱技术增强ROS 2系统的智能性，提高自主机器人任务的执行效率和决策能力。

Method: 提出了一套完整的方法论，包括：1）定义初始和目标条件；2）结构化任务和子任务；3）规划任务序列；4）在知识图谱中表示任务相关数据；5）使用高级语言设计任务。该方法在Aerostack2框架中实现，并在Gazebo环境中通过模拟搜救任务进行验证。

Result: 通过模拟搜救任务（无人机自主定位目标）的实践验证，证明了该方法能够有效提高决策能力和任务性能。知识图谱的应用使系统能够更好地理解和执行复杂任务。

Conclusion: 该论文提出的知识图谱集成方法能够显著增强ROS 2系统的智能性和任务执行效率，为自主机器人系统提供了一种有效的知识表示和任务规划框架，在复杂任务场景中具有重要应用价值。

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [68] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出基于神经风格迁移的sim-to-real强化学习策略转移方法，通过变分自编码器学习特征表示，生成弱配对轨迹数据，在机器人切割未知材料任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制中依赖大量仿真数据，但仿真与真实环境存在领域差距，且真实世界样本有限，限制了实际部署。

Method: 将神经风格迁移从图像处理重新解释到强化学习领域，使用变分自编码器联合学习自监督特征表示，生成弱配对的源-目标轨迹数据，提高合成轨迹的物理真实性。

Result: 相比基线方法（包括作者先前工作、CycleGAN和条件变分自编码器时间序列翻译），该方法在机器人切割未知材料任务中实现了更好的任务完成时间和行为稳定性，仅需少量真实世界数据。

Conclusion: 该方法对几何和材料变化具有鲁棒性，证明了在真实世界奖励信息不可用的接触密集型任务中策略适应的可行性。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
