<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同参数规模的SmolVLM2模型（500M和2.2B）在面向盲人和低视力用户的可访问性视频描述任务中的表现，并提出了两个专门针对BLV可访问性评估的新框架。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能够生成高质量的视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细、上下文感知描述的盲人和低视力用户。

Method: 评估了SmolVLM2的500M和2.2B参数变体在两个多样化数据集（AVCaps户外和Charades室内）上的表现，引入了两个新的评估框架：多上下文BLV框架和导航辅助框架，并系统评估了四种不同的提示设计策略。

Result: 在智能手机上部署了两种模型，评估了FP32和INT8精度变体，以评估在资源受限的移动设备上的实际性能约束。

Conclusion: 通过专门设计的评估框架和不同参数规模的模型比较，为开发更适合BLV用户需求的轻量级视频描述系统提供了重要见解。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自洽采样（SCS）方法，通过引入视觉扰动和重复截断重采样来解决多模态大语言模型中结果奖励强化学习面临的不可靠轨迹问题，显著提升多模态推理基准的准确性。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准的多选题设置中，结果奖励强化学习面临一个被忽视的障碍：即使推理链错误但最终猜对选项的不可靠轨迹，与真正正确推理的轨迹获得相同奖励，这严重影响了学习效果。

Method: 提出自洽采样（SCS）方法：对每个问题（i）引入小的视觉扰动，（ii）对初始轨迹进行重复截断和重采样；通过结果轨迹的一致性产生可微分的置信度分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准上准确率提升高达7.7个百分点，计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也获得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，有效解决了不可靠轨迹问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [3] [Robot Crash Course: Learning Soft and Stylized Falling](https://arxiv.org/abs/2511.10635)
*Pascal Strauch,David Müller,Sammy Christen,Agon Serifi,Ruben Grandia,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文提出了一种机器人无关的奖励函数，通过强化学习实现双足机器人的受控软着陆，平衡期望最终姿态与冲击最小化，并保护关键机器人部件。


<details>
  <summary>Details</summary>
Motivation: 尽管双足机器人在稳健运动方面取得进展，但在现实世界中仍有跌倒风险。大多数研究关注防止跌倒，而本文专注于跌倒现象本身，旨在减少机器人物理损伤并让用户控制机器人的最终姿态。

Method: 提出机器人无关的奖励函数，在强化学习中平衡期望最终姿态达成、冲击最小化和关键部件保护；引入基于模拟的初始和最终姿态采样策略，使策略对广泛初始跌倒条件具有鲁棒性，并能在推理时指定任意未见过的最终姿态。

Result: 通过模拟和真实世界实验证明，即使是双足机器人也能执行受控的软着陆。

Conclusion: 本文展示了双足机器人能够实现受控软着陆，为机器人安全操作提供了新的解决方案。

Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.

</details>
