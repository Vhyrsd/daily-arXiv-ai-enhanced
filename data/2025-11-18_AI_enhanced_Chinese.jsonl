{"id": "2511.11529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11529", "abs": "https://arxiv.org/abs/2511.11529", "authors": ["Luisa Mao", "Garret Warnell", "Peter Stone", "Joydeep Biswas"], "title": "Terrain Costmap Generation via Scaled Preference Conditioning", "comment": null, "summary": "Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.", "AI": {"tldr": "SPACER\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8d8a\u91ce\u5730\u5f62\u6210\u672c\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5b9e\u73b0\u5bf9\u65b0\u5730\u5f62\u7684\u826f\u597d\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u7684\u7f29\u653e\u504f\u597d\u4e0a\u4e0b\u6587\u5b9e\u73b0\u5feb\u901f\u6d4b\u8bd5\u65f6\u6210\u672c\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u652f\u6301\u5feb\u901f\u6d4b\u8bd5\u65f6\u6210\u672c\u8c03\u6574\uff08\u5982\u8bed\u4e49\u5206\u5272\uff09\uff0c\u8981\u4e48\u80fd\u6cdb\u5316\u5230\u65b0\u5730\u5f62\u7c7b\u578b\uff08\u5982\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u4f46\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u8fd9\u4e24\u4e2a\u76ee\u6807\u3002", "method": "SPACER\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6765\u6cdb\u5316\u5230\u65b0\u5730\u5f62\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u7684\u7f29\u653e\u504f\u597d\u4e0a\u4e0b\u6587\u5b9e\u73b0\u5feb\u901f\u6d4b\u8bd5\u65f6\u6210\u672c\u8c03\u6574\u3002", "result": "\u5728\u5927\u89c4\u6a21\u822a\u7a7a\u5730\u56fe\u4e0a\u7684\u5b9e\u8bc1\u8868\u660e\uff0cSPACER\u5728\u751f\u6210\u5730\u5f62\u5bfc\u822a\u6210\u672c\u56fe\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u57287\u4e2a\u73af\u5883\u4e2d\u76845\u4e2a\u5b9e\u73b0\u4e86\u5168\u5c40\u8def\u5f84\u89c4\u5212\u4e2d\u4e0d\u540c\u504f\u597d\u7684\u6700\u4f4e\u9057\u61be\u503c\u3002", "conclusion": "SPACER\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u826f\u597d\u7684\u5730\u5f62\u6cdb\u5316\u80fd\u529b\u548c\u5feb\u901f\u7684\u6d4b\u8bd5\u65f6\u6210\u672c\u8c03\u6574\uff0c\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11533", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11533", "abs": "https://arxiv.org/abs/2511.11533", "authors": ["Jueun Kwon", "Max M. Sun", "Todd Murphey"], "title": "Volumetric Ergodic Control", "comment": "8 pages, 8 figures", "summary": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f53\u79ef\u72b6\u6001\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u7a7a\u95f4\u8986\u76d6\u7684\u904d\u5386\u63a7\u5236\uff0c\u76f8\u6bd4\u4f20\u7edf\u70b9\u6a21\u578b\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301100%\u4efb\u52a1\u5b8c\u6210\u7387\u7684\u540c\u65f6\u5c06\u8986\u76d6\u6548\u7387\u63d0\u9ad8\u4e24\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u975e\u4f53\u79ef\u70b9\uff0c\u4f46\u5b9e\u9645\u4e2d\u673a\u5668\u4eba\u901a\u8fc7\u5177\u6709\u7269\u7406\u4f53\u79ef\u7684\u673a\u8eab\u548c\u4f20\u611f\u5668\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u4f53\u79ef\u8868\u793a\u3002", "method": "\u5f15\u5165\u65b0\u7684\u904d\u5386\u63a7\u5236\u516c\u5f0f\uff0c\u4f7f\u7528\u4f53\u79ef\u72b6\u6001\u8868\u793a\u4f18\u5316\u7a7a\u95f4\u8986\u76d6\uff0c\u652f\u6301\u4efb\u610f\u57fa\u4e8e\u91c7\u6837\u7684\u4f53\u79ef\u6a21\u578b\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u5728\u641c\u7d22\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u904d\u5386\u63a7\u5236\u65b9\u6cd5\uff0c\u8986\u76d6\u6548\u7387\u63d0\u9ad8\u4e24\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81c2\u6267\u884c\u673a\u68b0\u64e6\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4fdd\u6301\u4e86\u904d\u5386\u63a7\u5236\u7684\u6e10\u8fd1\u8986\u76d6\u4fdd\u8bc1\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\"\u653e\u5927\"\u8bc1\u636e\u6765\u7406\u89e3\u957f\u89c6\u89c9\u6587\u6863\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u89c9\u6587\u6863\u65f6\u9762\u4e34\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u7d22\u76f8\u5173\u9875\u9762\u548c\u6355\u6349\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u4e0a\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u65f6\uff0c\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u3002"}}
