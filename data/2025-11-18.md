<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/abs/2511.11552)
*Dawei Zhu,Rui Meng,Jiefeng Chen,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CV

TL;DR: DocLens是一个工具增强的多智能体框架，通过"放大"证据来理解长视觉文档，在MMLongBench-Doc和FinRAGBench-V上达到最先进性能，甚至超越人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理长视觉文档时面临证据定位的根本挑战，无法有效检索相关页面和捕捉视觉元素中的细粒度细节，导致性能有限和模型幻觉。

Method: 提出DocLens框架，首先从完整文档导航到相关页面上的特定视觉元素，然后采用采样-裁决机制生成单一可靠答案。

Result: 与Gemini-2.5-Pro配对，DocLens在MMLongBench-Doc和FinRAGBench-V上实现最先进性能，在视觉中心和不可回答查询上表现尤为突出。

Conclusion: DocLens通过增强的定位能力展示了其优势，特别是在处理视觉中心和不可回答查询时，证明了其强大的证据定位能力。

Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [2] [Terrain Costmap Generation via Scaled Preference Conditioning](https://arxiv.org/abs/2511.11529)
*Luisa Mao,Garret Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: SPACER是一种新颖的越野地形成本图生成方法，通过合成数据训练实现对新地形的良好泛化，并通过用户指定的缩放偏好上下文实现快速测试时成本调整。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么支持快速测试时成本调整（如语义分割），要么能泛化到新地形类型（如表示学习方法），但无法同时实现这两个目标。

Method: SPACER利用合成数据训练来泛化到新地形，并通过用户指定的缩放偏好上下文实现快速测试时成本调整。

Result: 在大规模航空地图上的实证表明，SPACER在生成地形导航成本图方面优于其他方法，在7个环境中的5个实现了全局路径规划中不同偏好的最低遗憾值。

Conclusion: SPACER能够同时实现良好的地形泛化能力和快速的测试时成本调整，为越野自主机器人导航提供了高质量的解决方案。

Abstract: Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.

</details>


### [3] [Volumetric Ergodic Control](https://arxiv.org/abs/2511.11533)
*Jueun Kwon,Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: 提出了一种新的体积状态表示方法，用于优化机器人空间覆盖的遍历控制，相比传统点模型方法，在保持100%任务完成率的同时将覆盖效率提高两倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有遍历控制方法将机器人建模为非体积点，但实际中机器人通过具有物理体积的机身和传感器与环境交互，需要更准确的体积表示。

Method: 引入新的遍历控制公式，使用体积状态表示优化空间覆盖，支持任意基于采样的体积模型，计算开销小适合实时控制。

Result: 在搜索和操作任务中，相比标准遍历控制方法，覆盖效率提高两倍以上，同时保持100%的任务完成率。

Conclusion: 该方法在机器人臂执行机械擦除任务中表现出有效性，保持了遍历控制的渐近覆盖保证，并显著提高了实际应用性能。

Abstract: Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.

</details>
